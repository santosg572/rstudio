---
title: "Introduction to Graph Theory"
---

Introduction to Graph Theory ............................................. 3 1.1 Definitions and Examples ............................................. 3 1.2 Spectral Graph Theory ................................................ 6 1.3 Centrality Measures ................................................... 8 1.3.1 Geometric Centralities ....................................... 9 1.3.2 Closeness ..................................................... 9 1.3.3 Path-Based Centralities ...................................... 10 1.3.4 Spectral Measures............................................ 12 1.4 Axioms for Centrality ................................................. 15 1.4.1 The Size Axiom .............................................. 16 1.4.2 The Density Axiom .......................................... 17 1.4.3 The Score-Monotonicity Axiom ............................ 17

## Chapter 1 Introduction to Graph Theory

### 1.1 Definitions and Examples

In this section, we give the definitions of **graphs**, **graphs’ properties**, and the data structures that serve to contain information on the graph nodes and topology and that are used by almost all graph analysis algorithms.

**Definition 1.1 (Graph)** An undirected graph is an ordered pair G = (V , E) of sets, where

E ⊂ {(v, w)\|v, w ∈ V,v = w}.

$$
E ⊂ \{ (v, w)|v, w ∈ V,v \neq w \}.
$$

The elements of V are called vertices (or nodes) of the graph G, and the elements of E are called edges. So in a graph G with vertex set $\{ v_1, v_2,...,v_n \}, (v_i, v_j ) ∈ E$ if and only if there is a line in G that connects the two vertices $v_i$ and $v_j$.

A directed graph is similar to an undirected graph, except for the fact that in a directed graph the elements of the edge set are ordered pairs of nodes, i.e., $E ∈ V × V$. The order of the pairs indicates the direction of the edge, e.g., $v_1, v_2 ∈ V × V$ , means that the edge connected $v_1$ to $v_2$ starts from v1 and ends in n2.

**Definition 1.2 (Complete Graph)** In a graph G = (V , E), two vertices $v_i, v_j ∈ V$ are adjacent or neighbours if $(v_i, v_j ) ∈ E$.

If all the vertices of G are pairwise adjacent, then we say G is complete. A complete graph with n vertices is denoted as $K^n$. For example, the graph of a triangle is $K^3$, the complete graph with three vertices. An undirected complete graph is a connected graph, but the vice versa is not true, see in this regard Fig. 1.1. Graph B. is connected, but it is not complete. Graph A. instead is complete.

**Definition 1.3 (Degree)** The degree $d(v)$ of a vertex v is the number of vertices in G that are adjacent to $v$.

![](images/Captura%20de%20pantalla%202024-12-11%20a%20la(s)%208.32.39.png){width="1176"}

In particular, out-degree of a node is the number of edges starting from it. The in-degree of a node is instead the number of edges ending to it. The total degree (simply said “degree”) of a node is then the sum of out- and in-degrees of it.

We can get two matrices from a graph G. One is called adjacency matrix, which we denote as $A_G$. The other is called Laplacian matrix, which we denote as LG. Let us assume that a graph G has the vertex set V = {1, 2,...,n}. The adjacency matrix, incidence matrix, and the Laplacian matrix of G are defined as follows:

**Definition 1.4 (Adjacency Matrix for an Undirected Graph)** The entries of the adjacency matrix $A_G$ of the graph G are

$$
 a_{ij}= \begin{cases}1 & \text{ if } (i,j) \in E\\ 0 & \text{ otherwise. } \end{cases}
$$

**Definition 1.5 (Adjacency Matrix for a Directed Graph)** The entries of the adjacency matrix $A_G$ of the graph G are

$$
 m_{ij}= \begin{cases} -1 & \text{ if } j \to i \\ 1 & \text{ if } i \to j \\
0 & \text{ otherwise.}
 \end{cases}
$$

**Definition 1.6 (Laplacian Matrix)** The entries of the Laplacian matrix $L_G$ of the graph G are

$$
 L_{ij}= \begin{cases} -1 & \text{ if } (i,j) \in E \\ d(i) & \text{ if } i = j \\
0 & \text{ otherwise.}
 \end{cases}
$$

For an undirected graph, the adjacency matrix is equal to the incidence matrix.

![](images/Captura%20de%20pantalla%202024-12-11%20a%20la(s)%208.50.22.png)

**Definition 1.7 (Incident Edge)** An edge $(v_j , v_j ) ∈ E$ is incident with a vertex $v ∈ V$ if $v = v_i$ or $v = v_j$.

**Definition 1.8 (Isomorphism)** Two graphs $G_1$ and $G_2$ are isomorphic if there is a one–one correspondence between the vertices of $G_1$ and those of $G_2$ such that the number of edges joining any two vertices of $G_1$ is equal to the number of edges joining the corresponding vertices of $G_2$.

Thus the two graphs shown in Fig. 1.2 are isomorphic under the correspondence $e ↔ n, f ↔ m, g ↔ h, a ↔ o, b ↔ i, c ↔ l$ \[207\].

**Definition 1.9 (Walk and Path)** A walk on a graph is a sequence of vertices that alternate with edges, beginning and ending with a vertex. In walk, each edge is incident with the vertex immediately preceding it and the vertex immediately following it. A path is a walk in which vertices are all distinct from each other.

**Theorem 1.1** A walk between two vertices $u$ and $v$ is called a $u − v$ walk. The length of a walk is the number of edges it has. For a graph G with vertex set $V = {1, 2,...,m}$, the entry an $a_{ij}^n$ of the matrix $A^n_G$ obtained by taking the nth power of the adjacency matrix $A_G$ equals the number of $i − j$ walks of length n.

**Proof** The theorem can be proved by induction: 1. Base step. When n = 1, aij = 1 if (i, j ) ∈ E. By definition, i(i, j )j is then an i −j walk of length 1 and this is the only one. So the statement is true for n = 1. 2. Inductive step. Now, we assume the statement is true for n = k and then prove the statement is also true for k + 1. Since Ak+1 ij = Ak ijAij ,

an+1 ij = m k=1 aik n akj . (1.1)

6 1 Introduction to Graph Theory aik n akj is the number of those walks i − j of length n joined by the edge (k, j ) because aki = 0 if (k, i) /∈ E and aki = 0 if (k, i) ∈ E. Noting that all walks from i to j of length n + 1 are of this form for some vertex k, an+1

ij in Eq. (8.54) is the total number of walks i − j of length n + 1. This proves the statements for n + 1.

We dedicate the end of this section to a special case of graph: the tree.

**Definition 1.10 (Tree)** A tree is a connected graph with no cycle.

If we remove any one edge from the spanning tree, it will make it disconnected. If we add one edge in a spanning tree, then it will create a cycle.

**Definition 1.11 (Spanning Graph and Spanning Tree)** A connected subgraph $G' = (V' , E' )$ of G is a spanning subgraph of G if $V' = V$ . It is a spanning tree if it is a tree and a spanning subgraph of G. A disconnected graph does not have any spanning tree.

**Definition 1.12 (Complexity)** The complexity of a graph G, denoted with $κ(G)$, is the number of spanning trees of G.

If a graph is a complete graph G with \|V \| vertices, its complexity is

$$
κ(G) = |V|^{ (|V |−2)}  
$$

. (1.2)

The formula (1.2) is known as Cayley’s formula. If a graph is connected but not complete, the number of spanning trees is equal to any cofactor of the Laplacian matrix (Kirchhoff’s theorem).

Finally, we recall the following properties hold:

-   All spanning trees have \|V \| − 1 edges.

-   To find a spanning tree from a selected node, we can use the minimum spanning tree on an unweighted graph.

-   A disconnected graph does not have any spanning tree.

### 1.2 Spectral Graph Theory

Spectral graph theory is the study of properties of the Laplacian matrix or adjacency matrix associated with a graph. This theory is of particular interest because it highlights the connection between the eigenvalues of the Laplacian matrix and graph connectivity.

Following what was suggested in Jiang \[109\], we should consider a definition of the Laplacian that differs from (but is equivalent to) Definition 1.1 given in the previous section. Jiang proposes the following definition.

**Definition 1.13** For a graph G = (V.E),

$$
L_G = \sum_{\{u,v\}∈E} l_{G_{\{u,v\}}}.
$$

Using this definition, it is easy to verify that $L_G$ is a self-adjoint matrix, i.e., $L_G$ is equal to its conjugate transpose, and consequently, its eigenvalues are all real. Moreover, $L_G$ is positive semi-definite, i.e., its eigenvalues are all positive.

With the help of this definition, we will show how the Laplacian contains the information concerning the connectivity of a graph. We first recall that a path is in fact a walk with no repeating vertices and the following definition.

**Definition 1.14** A non-empty graph G is called connected if any two of its vertices are contained in a path in G.

The eigenvalue 0 of the graph Laplacian is related to the connectedness of the graph, as stated by the Lemma 1.1 This Lemma has been proved by J. Jiang in \[109\]. We report here the proof of J. Jiang using the same notation of the author.

**Lemma 1.1** For any graph G, $λ_1 = 0$ for $L_G$. If G = (V , E) is a connected graph where V = {1, 2,...,n}, then $λ_2 > 0$.

**Proof** Let x = (1, 1,..., 1) ∈ Rn. Then the entry m1 of the matrix M = LGx is

m1 = n k=1 l1k.

From Definition 1.1 of LG, it follows that mi = 0 since the row entries of LG add up to zero. So LGx

= 0. Therefore, 0 is an eigenvalue of LG. Since 0 ≤ λ1 ≤ λ2 ≤

···≤ λn, it follows that λ1 = 0. To show that λ2 \> 0 for a connected graph, we proceed in the following way. Let z be a nonzero eigenvector of 0. Then z T LGz = zT · 0 = 0.

So

# zT LGz

{u,v}∈E (zu − z∇) 2 = 0.

This implies that for any {u, v} such that {u, v} ∈ E, zu = zv. Since G is connected, this means zi = zj for all i, j ∈ V . Therefore,

z = α ⎛ ⎜ ⎜ ⎜ ⎝ 1 1 . . . 1 ⎞ ⎟ ⎟ ⎟ ⎠ ,

8 1 Introduction to Graph Theory where α is some real number. So,

Uλ1 = Span((1, 1,..., 1)),

where Uλ1 is the eigenspace of λ1. Therefore, the multiplicity of eigenvalue 0 is 1. It follows that λ2 = 0,so2 \> 0.

Indeed, the multiplicity of the eigenvalue 0 of $L_G$ tells us the number of connected components in the graph G. Let us define rigorously what a connected component of a graph is.

**Definition 1.15** A connected component of a graph $G = (V , E)$ is a subgraph $G' = (V' , E')$, $(V' ⊂ V, E' = \{ \{x, y \} ∈ E | x, y ∈ V' \} )$, in which any two vertices $i, j ∈ V$ are connected, while for any $i ∈ V'$ and $k ∈ V \V'$, i, k are not connected.

From Lemma 1.1, Corollary 1.1 follows.

**Corollary 1.1** Let $G = (V , E)$ be a graph. Then the multiplicity of 0 as an eigenvalue of $L_G$ equals the number of connected components of G.

**Proof** Suppose G1 = (V1, E1), G2 = (V2, E2),...,Gk = (Vk, Ek) are the connected components of G. Let w

i be defined by (w1)j =

1 if j ∈ Vi 0 otherwise .

Then, it follows from the previous lemma that if x ∈ Rn is a nonzero eigenvector of 0, then xi = xj for any (i.j ) ∈ V such that i, j are in the same connected component. So

Uλ1 = Span({w1, w2,...,wk})

w1, w2....,wk are linearly independent, and therefore, the multiplicity of the null eigenvalue of LG is the number of connected components in G.

### 1.3 Centrality Measures

Node centrality measures can be classified into three categories: geometric cen- trality measures, path-based centrality measures, spectral measures, and dynamic measures. Measures that belong to the first category quantify the importance of a node based on the geometry of its connections with its first neighbours; measures of centrality that belong to the second category describe and quantify the importance of a node based on the quantitative properties of the paths that pass through it; measures that belong to the third category are derived from the spectral properties of graph matrices; finally, the central measures belonging to the fourth category quantify the importance of a node based on the intensity of the node’s response to external stresses exerted on the node.

### 1.3.1 Geometric Centralities

Among the most commonly used measures of geometric centrality are the clustering coefficient, the closeness, the Lin’s index, and the harmonic centrality. Here is a brief description and reference to the author’s papers that introduced them.

#### 1.3.1.1 Clustering Coefficient

The clustering coefficient \[206\] $C_i$ of a vertex i is the frequency of pairs of neighbours of i that are connected by an edge, that is, it is the ratio of the number mi of pairs of neighbours of i that are connected and the number of possible pairs of neighbours of i, which is $k_i(k_i − 1)/2$, where $k_i$ is the degree of i:

$$
C_i = \frac{2m_i}{k_i(k_i − 1)}  
$$

$C_i$ is said to be local clustering coefficient and can be interpreted as the probability that a pair of neighbours of i are connected. It quantifies how close its neighbours are to being a clique (complete graph).

Two versions of this measure exist: the global and the local. The global version was designed to give an overall indication of the clustering in the network, whereas the local gives an indication of the embeddedness of single nodes.

**1.3.2 Closeness**

The closeness centrality of node i was introduced by Bavelas \[16\] and is defined by

$$
S_i = \frac{1}{\sum_j d(i,j)}
$$

where d(i, j ) is the distance between node i and node j , i.e., the number of edges in a shortest path connecting them. Nodes with a high closeness score have the shortest distances to all other nodes. It is assumed that nodes with an empty co-reachable set have closeness centrality 0 by definition.

The normalized form of the closeness centrality is the average length of the shortest paths between two nodes. It is given by

$$
S_i = \frac{N − 1}{\sum_j d(i, j )}, 
$$

where N is the number of nodes in the graph.

#### 1.3.2.1 Lin’s Index

Nan Lin \[133\] reformulated the definition of closeness provided by Bavelas \[16\] for graphs with infinite distances by weighting closeness using the square of the number of co-reachable nodes and provided this definition for the centrality of a node i with a non-empty co-reachable:

$$
\frac{|\{j | d(i, j ) < ∞ \}|^2}{\sum_{d(i,j )<∞} d(i, j)} .
$$

**1.3.2.2 Harmonic Centrality**

The harmonic centrality of a node i proposed by \[138\] is the denormalized reciprocal of the harmonic mean of all distances (even ∞) from i, i.e.: We thus define the harmonic centrality of x as

$$
\sum_{i \neq j} \frac{1}{d(i, j )} = \sum_{d(i,j )<∞,i\neq j} \frac{1}{d(i, j )}. 
$$

This measure is useful in the case in which in a graph a large number of pairs of nodes are not reachable. In that case, the average of finite distances can be misleading, as a graph might have a very low average distance, while it is almost completely disconnected.

#### 1.3.3 Path-Based Centralities

Path-based centrality measures include betweenness centrality, subgraph centrality, and information centrality that are currently the most used centrality measures to define the ability of each node to exert influence on the flow of information. These centrality indices and their comparison on large networks have received much attention from the biological science community. As reported in \[75\], for example, in protein–protein interaction networks, a comparison of path-based centrality measures showed proteins with phenotypic importance during adaptation to shifting nitrogen environments.

#### 1.3.3.1 Betweenness Centrality

Betweenness centrality \[68\] quantifies the relative importance of a node in the com- munication between other pairs of nodes. Nodes with high betweenness centrality highly facilitate or highly inhibit the communication between other nodes in the network.

Assuming that the information from one node to another node travels only through the shortest paths connecting those nodes, we can express mathematically the betweenness centrality of a node i as

$$
BC_i = \sum_i \sum_j \frac{ρ(i, k, j )}{ρ(i, j )} ,
$$

where $ρ(i, j )$ is the number of shortest paths from node i to node j , and ρ(i, k, j ) is the number of these shortest paths that pass through node k.

### 1.3.3.2 Subgraph Centrality

The subgraph centrality \[60\] of the node i is the number of closed walks of different lengths in the network starting and ending at node i. Mathematically, the number of closed walks of length L starting and ending on node i is given by the i-th diagonal entry of the L-th power of the adjacency matrix A:

$$
SC_i = (A^L)_{ii}.
$$

Subgraph centrality characterizes the participation of each node in all subgraphs in a network. Smaller subgraphs are given more weight than larger ones, so that subgraph centrality measure is appropriate for characterizing network motifs.

### 1.3.3.3 Information Centrality

The weighted function of the set of all paths connecting node i to node j is any weighted linear combination of the paths such that the sum of the weights is unity. Assuming that each link in a path is independent, and the variance of a single link is unity, it can be concluded that the variance of a path is simply its length.

The information measure between two nodes i and j is the inverse of the variance of the weighted function. Mathematically, it is defined as the harmonic mean of all the information measures between i and all other nodes in the network

$$
IC_i = [ \frac{1}{n} \sum_j \frac{1}{I_{ij}} ]
$$

ICi =

1 n

j 1 Iij ,

where n is the number of nodes, $I_{ij} = (c_{ii}+c_{jj} −c_{ij} )^{-1}$, and $C = (c_{ij} )$ = D−A+J with D the degree matrix, A the adjacency matrix, and J a matrix with all elements equal to 1.

#### 1.3.4 Spectral Measures

To this category belong centrality measures based on the spectral properties of the matrices describing a graph, such as the Laplacian matrix.

#### 1.3.4.1 Eigenvector Centrality

Eigenvector centrality is an extension of degree centrality. In-degree centrality awards one centrality point for every link a node receives. However, since not all vertices are equivalent (some are more important than others), and, reasonably, endorsements from important nodes count more. Eigenvector centrality assumes the thesis by which “a node is important if it is linked to by other important nodes”.

Let $A = (a_i,)$ be the adjacency matrix of a graph (i.e., $a_{ij} = 1$ if node i is linked to vertex j , and $a_{ij} = 0$ otherwise). The eigenvector centrality $EV_i$ of node i is recursively defined:

$$
EV_i = \frac{1}{λ} \sum_{k∈N(i)} a_{ki}EV_k
$$

where $N (i)$ is the set of neighbours of i, and initial vertex centrality $EV_k = 1, ∀k$ including i. $λ \neq 0$ is a constant such that $λx = Ax$.

### 1.3.4.2 Hub Centrality

The hub scores (also known as Kleinberg centralities) of the nodes are defined as the principal eigenvector of $AA^T$, where A is the adjacency matrix of the graph. Kleinberg centrality \[118\] is a generalization of eigenvector centrality that prevents the problem of ordinary eigenvector centrality on directed networks, that nodes outside strongly connected components, or their out-components get null centrality.

### 1.3.4.3 Katz’s Index

The fact that eigenvector centrality only functions well when the graph is (strongly)

connected is a practical issue. A strongly connected component, whose size is pro- portionate to the network’s size, is typical of real undirected networks. Real directed networks, however, do not. Only vertices in strongly connected components or in the out-component of such components can have nonzero eigenvector centrality if a directed network is not strongly connected. The null centrality of all the remaining vertices, even those in the in-components of strongly connected components, is mostly unjustified. This occurs because nodes with no incoming edges have nodes that are pointed to solely by nodes with a null centrality score, which are themselves defined to have nodes with a null eigenvector centrality score.

Giving each node a modest amount of free centrality, irrespective of the vertex’s location in the network, is one technique to get around this issue. Each node, then, has a minimum, positive quantity of centrality that it can refer to other nodes and transfer to them. In particular, linked nodes have higher centrality than unlinked nodes, the centrality of which is exactly this minimum positive quantity. Therefore, independent of the centrality of the linkers, strongly linked nodes have high centrality. However, if the linkers have high centrality, even nodes that receive few links may still have high centrality. This method has been proposed by L. Katz \[115\].

Let the adjacency matrix of a directed graph be $A = \{a_{i,j} \}$. The formula for the Katz centrality of a node i is

$$
K_i = \sum^∞_{h=1} \sum^n_{j=1} α^h (a_{ij})^k + β,  
$$

where α and β are two constants. From the definition, we see that the centrality vector K is defined by two components: an endogenous component (\$\\sum\^∞\_{h=1}\\sum\^n\_{j=1}α\^h(a\_{ij})\^k\$) taking into consideration the network topology represented by the adjacency matrix and an exogenous component that is independent of the network structure (β). It follows that, in matrix form, the Katz centrality vector K can be computed as

$$
K = β(I − αA)^{-1} = β \sum^∞_{h=0}(αA)^h,  
$$

where α and β are vectors. The formula (1.8) states that the Katz index of a node is the number of weighted paths reaching the node in the network plus an exogenous factor, a generalization of the in-degree measure that counts only paths of length one. By adopting the attenuation factor α (known as damping factor), long paths are weighted less than short ones. In this way, over long chains of links, endorsement lose value. As α decreases, the contribution of path longer than one decreases, and Katz’s index is dominated by short path (mostly in-degree) and by the exogenous factor of the system. When α is large, long paths are devalued smoothly, and Katz’s index is more influenced by the endogenous topological part of the system. It is recommended to choose α between 0 and $1/λ_{max}$ \[67, 113\], where λmax is the largest eigenvalue of A (the measure diverges at $α = 1/λ_{max}$).

#### 1.3.4.4 Vibrational Centrality

A network’s vulnerable nodes must be identified for a variety of practical reasons. It enables, for example, the study of the preservation of ecosystem functionalities or the building of robust infrastructure networks and the efficient elimination of malevolent networks. Approaches to assessing network susceptibility to external changes, or changes irrespective of network structure, are particularly important \[98\]. Examples include intermolecular interactions, whose structure varies signif- icantly depending on whether a substance is in a solid, liquid, or gaseous state of matter. Here we report the Estrada et al. \[58, 59\] model of node vulnerability. According to the Estrada’s model, a network immersed in a thermal bath at temperature T can be used to simulate a network that has been subjected to some form of external stress. The network is thought to be perturbed by the heat variations. The vibrational potential energy of the network can be expressed as follows, assuming that xi represents the displacement of node I from its static position:

$$
V(x) = \frac{k}{2} x^TLx,  
$$

where k is the spring constant and x is the vector of node displacements over the network. The probability distribution of the displacement of the nodes is defined by the Boltzmann distribution

$$
P(x)=\frac{e^{-\frac{V(x)}{T}}}{Z} = \frac{1}{z} \exp (- \frac{k}{2T} x^T Lx),
$$

(1.10)

where the partition function Z of the network is defined as follows:

$$
Z = \int dx \exp (- \frac{k}{2T} x^T Lx 
$$

The mean displacement of a node i is by definition

$$
Δx_i = \sqrt{\int x^2_i P(x) dx}
$$

Δxi =x2 i P (x) dx. (1.12)

It can be shown that \[58, 59\]

$$
Δx_i = \sqrt{\sum^N_{j=2} \frac{U^2_{ij}}{βkλ_j}} = \sqrt{\frac{T}{k}(L^{+})_{ii}}, 
$$

where U is the matrix whose columns are the orthonormal eigenvectors, λ the eigenvalues of the Laplacian matrix L, and $L^{+}$ the pseudo-inverse of L. Node displacement depends on the eigenvectors corresponding to all non-trivial eigenvalues of the Laplacian matrix. Nodes displaying higher values of the displacement are more vulnerable to changes in external conditions. We can understand this interpretation by analysing the displacement in terms of the Laplacian spectrum. Let us consider $0 = λ_1 < λ_2 < ··· < λ_N$ . Since the eigenvalues of the Laplacian appear in the denominator of node displacement, the term $\frac{U^2_{i2}}{λ_2}$ has the largest contribution to Δxi. The smallest non-trivial eigenvalue and its corresponding eigenvector are known respectively as the algebraic connectivity and the Fiedler vector of the network. Accordingly, the nodes of the network are partitioned into two sets $V_1$ and $V_2$, as follows: $V_1 = \{ i | U_{i,2} < 0 \}, V_2 = \{ i | U_{i,2} ≥ 0 \}$. That is, a node belongs strongly to a partition if it has a large positive or a large negative value of the corresponding entry in the Fiedler vector, whereas a node with $U_{i,2}$ close to zero does not belong strongly to either partition. As a result, the latter nodes might be regarded as nodes of articulation between the partitions. Such articulation nodes are removed, isolating the positive and negative vector components’ positive and negative partitions. It is important to note that algebraic connectedness represents how strong a network is; the higher the algebraic connectivity, the more challenging it is to isolate individual components of a network. Then, the two main contributions to the displacement make sure that the nodes that mostly aid in dividing the network into detached components correspond to the nodes with the least displacements.

Compared to topological measurements such as node degree, vibrational central- ity was found to yield excellent resolution in the identification of vulnerable nodes \[59\].

#### 1.4 Axioms for Centrality

For many different reasons, comparing centrality measurements is a tough, challeng- ing, and laborious endeavour. Here we report an interesting analysis of this problem and an attempt of axiomatization by Boldi et al. \[24\].

The authors in \[24\] recommend using a set of axioms to explain how a centrality metric behaves. They assume that the centrality metrics under investigation are isomorphically invariant, which means that they depend solely on the structure of the graph and not on the particular labelling scheme selected for each node. They advise looking at how centrality measurements react to variations in size, density, and arc additions in order to abide by these limitations. Nodes having a denser neighbourhood should likewise be more significant when every other parameter is fixed, as should nodes belonging to larger groups.

How can we genuinely tell if this occurs precisely and perhaps in an asymptotic environment? To achieve this, we must try something completely novel by exactly (i.e., in algebraic closed form) evaluating all relevant measures on each node of selected typical networks. Selecting two graphs that are at opposite ends of the density spectrum should better illustrate how centrality measurements respond to density. Additionally, there are directed p-cycles and k-cliques for every k and p. (This might not happen for more complicated structures, e.g., a cubic graph.)

In order that this happens precisely and maybe in an asymptotic setting, according to Bold’s suggestion, we must take a completely innovative approach: assessing all pertinent measures on each node of selected typical networks precisely (i.e., in algebraic closed form). It should be easier to understand how centrality measurements respond to density if two graphs are chosen that are at opposingextremities of the density spectrum. For every k and p, there are also directed p- cycles and k-cliques (this might not happen for more complicated structures, e.g., acubic graph).

Among the centrality measures listed in the previous sections, according to the analysis of Boldi et al. \[24\], to only harmonic centrality, turns out to satisfy all axioms.

#### 1.4.1 The Size Axiom

Let us consider a graph made by a k-clique and a p-cycle. A clique is a collection of vertices in an undirected graph G such that every two different vertices in the clique are nearby, implying that the induced subgraph is complete. All nodes of the clique and the cycle have equal scores due to isomorphism’s invariance; if p = k, the elements on the clique are more significant. This axiom is trivial and can be satisfied by almost any measure that we are aware of, but if we are interested in determining the sensitivity to size, the first axiom is as follows:

**Definition 1.16 (Size Axiom \[24\])** Consider the graph Sk,p made by a k-clique and a directed p-cycle. A centrality measure satisfies the size axiom if for every k there is a Pk such that for all p ≥ Pk in Sk,p the centrality of a node of the p-cycle is strictly larger than the centrality of a node of the k-clique, and if for every p, there is a Kp such that for all k ≥ Kp in Sk,p the centrality of a node of the k-clique is strictly larger than the centrality of a node of the p-cycle.

Nodes of the cycle will be less significant than nodes of the clique when p = k. The reasoning behind the case k → ∞ is rather straightforward: since the denser community is also growing, its members are anticipated to eventually become even more significant. On the other hand, if the cycle grows extremely large (more specifically, if its size reaches infinity), its nodes are still a part of a very large (albeit poorly connected) community, and we anticipate that they will eventually become more important than the nodes of a fixed-size community, regardless of how dense it may be.

**1.4.2 The Density Axiom**

Since we must be able to specify a rise in density “with all other parameters fixed”, including size, creating an axiom for density is a more challenging task. Let us begin by connecting a node x of the k-cycle with a node y of the p-cycle through a bidirectional arc, or the bridge, starting from a graph made up of a directed k- cycle and a directed p-cycle. The vertices x and y must necessarily have the same score if k = p because they are symmetric. The k-cycle is now transformed into a k- clique by increasing its density as much as possible. Because the degree of y has not changed, it should be noted that this change in density is limited to x. By keeping all other parameters constant while tightly increasing the local density around x, we can assume that the x score will rise.

**Definition 1.17 (Density Axiom \[24\])** Consider the graph $D_{k,p}$ made by a k-clique and a p-cycle (p, k ≥ 3) connected by a bidirectional bridge $x ↔ y$, where x is a node of the clique and y is a node of the cycle. A centrality measure satisfies the density axiom if for k = p the centrality of x is strictly larger than the centrality of y.

Note that our axiom does not specify any constraint when $k \neq p$. While studying the behaviour of the graph $D_{k,p}$ of the previous definition when $k \neq p$ shades some lights of the inner behaviour of centrality measures, it is essential, in an axiom asserting the sensitivity to density, that size is not involved.

**1.4.3 The Score-Monotonicity Axiom**

This axiom specifies strictly monotonic behaviour upon the addition of an edge:

**Definition 1.18 (Score-Monotonicity Axiom \[24\])** A centrality measure satisfies the score-monotonicity axiom if for every graph G and every pair of nodes x, y such that x y, when we add $x → y$ to G, the centrality of y increases.

Every centrality metric we consider for strongly linked graphs satisfies this axiom. It is a suitable approach to verify that a measure can handle partially disconnected graphs as a result. The reader may define a weak monotonicity axiom that just demands that y’s rank not drop. The centrality metric, which gives each network node a fixed value, would satisfy this axiom if employed alone, rendering it uninteresting for our goals.

Paola Lecca • Bruno Carpentieri

Introduction to Mathematics for Computational Biology
