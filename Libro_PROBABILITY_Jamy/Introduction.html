<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="Introduction_files/libs/clipboard/clipboard.min.js"></script>
<script src="Introduction_files/libs/quarto-html/quarto.js"></script>
<script src="Introduction_files/libs/quarto-html/popper.min.js"></script>
<script src="Introduction_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Introduction_files/libs/quarto-html/anchor.min.js"></script>
<link href="Introduction_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Introduction_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Introduction_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Introduction_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Introduction_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>This chapter introduces the basic concepts of probability theory. These are the notions of:</p>
<ul>
<li><p>an outcome space, or set of all possible outcomes of some kind;</p></li>
<li><p>events represented mathematically as subsets of an outcome space; and</p></li>
<li><p>probability as a function of these events or subsets.</p>
<p>The word “event” is used here for the kind of thing that has a probability, like getting a six when you roll a die, or getting five heads in a row when you toss a coin five times. The probability of an event is a measure of the likelihood or chance that the event occurs, on a scale from 0 to 1. Section 1.1 introduces these ideas in the simplest setting of equally likely outcomes. Section 1.2 treats two important interpretations of probability: approximation of long-run frequencies and subjective judgment of uncertainty. However prohabilities are understood or interpreted, it is generally agreed that they must satisfy certain rules, in palticular the basic addition rule. This rule is huilt in to the idea of a probability distribution, introduced in Section 1.3. The concepts of conditional probability, and independence appear in Section 1.4. These concepts are further developed in Section 1.5 on Bayes’ rule and Section 1.6 on sequences of events.</p></li>
</ul>
<p>1.1 Equally Likely Outcomes Probability is an extension of the idea of a proportion, or ratio of a part to a whole. If there are 300 men and 700 women in a group, the proportion of men in the group is</p>
<p><span class="math display">\[
\frac{300}{300 + 700} = 0.3 = 30%
\]</span></p>
<p>Suppose now that someone is picked at random from this population of men and women. For example, the choice could be made by drawing at random from a box of 1000 tickets, with different tickets corresponding to different people. It would then be said that</p>
<ul>
<li><p>the probability of choosing a woman is 70%;</p></li>
<li><p>the odds in favor of choosing a woman are 7 to 3 (or 7/3 to 1); and</p></li>
<li><p>the odds against choosing a woman are 3 to 7 (or 3/7 to 1).</p>
<p>So in thinking about someone picked at random from a population, a proportion in the population becomes a probability, and something like a sex ratio becomes an odds ratio.</p>
<p>There is an implicit assumption here: “picked at random” means everyone has the same chance of being chosen. In practice, for a draw at random from a box, this means the tickets are similar, and well mixed up before the draw. Intuitively, we say different tickets are equally likely, or that they have the same chance. In other words, the draw is honest, fair, or unbiased. In more mathematical language, the probability of each ticket is the same, namely, 1/1000 for an assumed total of 1000 tickets.</p>
<p>For the moment, take for granted this intuitive idea of equally likely outcomes. Represent the set of all possible outcomes of some situation or experiment by n (capital omega, the last letter in the Greek alphabet). For instance, n would be the set of 1000 people (or the 1000 corresponding tickets) in the previous example. Or n = {head, tail} for the result of tossing a coin, or n = {I, 2, 3, 4, 5, 6} for rolling an ordinary six-sided die. The set n is called the outcome space. Something that might or might not happen, depending on the outcome, is called an event. Examples of events are “person chosen at random is a woman”, “coin lands heads”, “die shows an even number”. An event A is represented mathematically by a subset of the outcome space n.&nbsp;For the examples above, A would be the set of women in the population, the set comprising the single outcome {head}, and the set of even numbers {2, 4, 6}.</p>
<p>Let #(A) be the number of outcomes in A. Informally, this is the number of chances for A to occur, or the number of different ways A can happen. Assuming equally likely outcomes, the probability of A, denoted P(A), is defined to be the corresponding proportion of outcomes. This would be 700/1000, 1/2, and 3/6 in the three examples.</p></li>
</ul>
<p>Equally Likely Outcomes If all outcomes in a finite set f2 are equally likely, the probability of A is the number of outcomes in A divided by the total number of outcomes: P(A) = #(A) #(f2) Probabilities defined by this formula for equally likely outcomes are fractions between 0 and 1. The number 1 represents certainty: P(f2) = 1. The number 0 represents impossibility: P(A) = 0 if there is no way that A could happen. Then A corresponds to the empty set, or set with no elements, denoted 0. So P(0) = O. Intermediate probabilities may be understood as various degrees of certainty. Picking a number between 1 and 100. Suppose there is a box of 100 tickets marked 1,2,3, ... , 100. A ticket is drawn at random from the box. Here are some events, with their descriptions as subsets and their probabilities obtained by counting. All possible numbers are assumed equally likely. Subset of {I, 2, ... , 100} Probability the number drawn has one digit {I, 2, ... ,9} 9% the number drawn has two digits {10, 11, ... , 99} 90% the number drawn is less than or equal to the number k {1,2, ... ,k} k% the number drawn is strictly greater than k {k + 1, ... , 100} (100 - k)% the sum of the digits in the number drawn is equal to 3 {3,12,21,30} 4% Example 2. Problem 1. Solution. Rolling two dice. A fair die is rolled and the number on the top face is noted. Then another fair die is rolled, and the number on its top face is noted. What is the probability that the sum of the two numbers showing is 5? Think of each possible outcome as a pair of numbers. The first element of the pair is the first number rolled, and the second element is the second number rolled. The first number can be any integer between 1 and 6, and so can the second number. Here are all the possible ways the dice could roll:</p>
<p>(1,1) (1,2) 0,3) (1,4) 0,5) (1,6) (2,1) (2,2) (2,3) (2,4) (2,5) (2,6) (3,1) (3,2) (3,3) (3,4) (3,5) (3,6) (4,1) (4,2) (4,3) (4,4) (4,5) (4,6) (.5,1) (5,2) (5,3) (5,4) (5,5) (5,6) (6,1) (6,2) (6,3) (6,4) (6,5) (6,6) The collection of these 36 pairs forms the outcome space n.&nbsp;Assume these 36 outcomes are equally likely. The event “the sum of the two numbers showing is 5” is represented by the subset {(1,4), (4, I), (2,3), (3,2)}. Since this subset has 4 elements, 4 1 P(sum of two numbers showing is 5) = 36 = 9 What is the probability that one of the dice shows 2, and the other shows 4? The subset corresponding to this event is {(2, 4), (4, 2)}. So the required probability is 2/36 = 1/18. What is the probability that the second number rolled is greater than the first number? Look at the pairs in the outcome space n above, to see that this event corresponds to the subset (1,2) (1,3) 0,4) 0,5) (1,6) (2,3) (2,4) (2,5) (2,6) (3,4) (3,5) (3,6) (4,5) (4,6) (5,6) These are the pairs above the diagonal in n.&nbsp;There are 15 such pairs, so the probability that the second number rolled is greater than the first is 15/36. What is the probability that the second number rolled is less than the first number rolled? The subset of n corresponding to this event is the set of pairs below the diagonal. There are just as many pairs below the diagonal as above. So the probability that the second number rolled is less than the first number is &lt;,llso 15/36. Rolling two n-sided dice. Repeat the above example for two rolls of a die with n faces numbered 1,2, ... , n, assuming n 2: 4. Find the chance that the sum is 5.</p>
<p>Solution. Problem 2. Solution. Problem 3. Solution. Remark. Section 1.1. Equally Likely Outcomes 5 Now there are n2 possible pairs instead of 62 = 36. But there are still just 4 possible pairs with sum 5. Hence P(sum is 5) = 4/n2 Find the chance that one roll is a 2, the other is a 4. By the same argument P(a 2 and a 4) = 2/n2. Find the chance that the second number is greater than the first. Now all pairs above the diagonal must be counted in an n x n matrix of pairs. There are no such pairs in the bottom row, 1 in the next, 2 in the next, and so on up to (n - 1) pairs in the top row, so the number of pairs above the diagonal is 1 #(above) = 1 + 2 + 3 + ... + (n - 1) = 2”n(n - 1) pairs altogether (see Appendix 2 on sums.) This gives #(above) In(n - 1) 1 ( 1) P(second number is greater) = #( I) = 2 2 = - 1 - - tota n 2 n Here is another way to find #(above), which gives the formula for the sum of the first n - 1 integers (used above) as a consequence. Since #(below) + #(above) + #(diagonal) = #(total) = n2 and #(below) = #(above) by symmetry, and #(diagonaJ) = n, 1 #(above) = (n2 - n)/2 = 2”n(n - 1) Problem 4. Find the chance that the first number is bigger. Solution. Same as above, by the symmetry used already. Note. As n --+ 00, # (diagonal) n 1 P(two numbers are equal) = ( ) = 2’ = - --+ 0 # total n n hence P(second bigger) = P(first bigger) = ~ (1 - ~) --+ ~ 2 n 2</p>
<p>Odds In a setting of equally likely outcomes, odds in favor of A give the ratio of the number of ways that A happens to the number of ways that A does not happen. The same ratio is obtained using probabilities instead of numbers of ways. Odds against A give the inverse ratio. More generally, just about any ratio of chances or probabilities can be called an odds ratio. Gamblers are concerned with another sort of odds, which must be distinguished from odds defined as a ratio of chances. These are the odds offered by a casino or bookmaker in a betting contract, called here payoff odds to make the distinction clear. If you place a $1 bet on an event A, and the payoff odds against A are 10 to 1, you stand to win $10 if A occurs, and lose your $1 if A does not occur. In a casino you first pay your $1. If A occurs you get back a total of $11. This is your winnings of $10 plus your $1 back. If A does not occur, the casino keeps your $1. The price of $1 is your stake, the $10 is the casino’s stake, and the $11 is the total stake. The connection between payoff odds and chance odds is an ancient principle of gambling, understood long before mathematicians decided that probabilities were best represented as numbers between 0 and 1. Around 1584, a colorful gambler and scholar of the Italian Renaissance, named Cardano, wrote a book on games of chance. Considering the roll of a die, Cardano said, I am as able to throw a 1, 3 or 5 as 2, 4 or 6. The wagers are therefore laid in accordance with this equality if the die is honest, and if not, they are made so much the larger or smaller in proportion to the departure from true equality. First there is the idea of equally likely outcomes, then a heuristic connecting payoff odds and chance odds: The Fair Odds Rule In a fair bet, the payoff odds equal the chance odds. That is to say, in a fair bet on an event A, where you win if A occurs and the casino wins otherwise, the ratio of your stake to the casino’s stake should be the ratio of probabilities P(A) to 1 - P(A). Put another way, your stake should be proportion P(A) of the total stake.</p>
<p>Example 4. Section 1.1. Equally Likely Outcomes 7 House percentage at roulette. A Nevada roulette wheel has equally spaced pockets around its circumference. The pockets are numbered 1 through 36, 0 and 00. The wheel is spun, a ball inside the wheel is released, and by the time the motion stops the ball has been caught in one of the pockets. A play is a bet that the ball will fall in one of a certain set of pockets, with the payoff odds as shown below in Figure 1. FIGURE 1. Layout of a Nevada roulette table. Key to colors 0 and 00 = Green, unshoded numbers = Red, shaded numbers = Block.</p>
<p>chance odds against a play, just take the ratio of losing numbers to winning numbers for that play. For example, the 38 numbers are divided into 18 numbers which are red, 18 numbers which are black, and two numbers are green (0 and 00). So the chance odds against red are 20 to 18, as are the chance odds against black. Put another way, P(red) = P(black) = 18/38 The house offers bets on red at even odds, that is to say, payoff odds of 1 to 1. You can think about this in the following way. Suppose you stake $1 on red. The casino then has a stake of $1 on not red. The total stake in the game is $1 + $1 = $2. According to the fair odds rule, the fair price to pay would be proportion P(red) of the total stake, that is, eighteen thirty-eighths of $2. The $1 you pay exceeds the fair price by ( 18 ) 1 $1 - 38 x $2 = $19 = 5.26% of $1 = 5.26 cents So this is not a fair bet. The figure of 5.26% is called the house percentage for bets on red at roulette. Assuming red comes up about 18/38 of the time over the long run (something which casinos take great care to ensure), this means that if you repeatedly bet a dollar on red, the house would be taking money from you at an average rate of 5.26 cents a game. If you bet a dollar 100 times in a row, you can expect to lose $5.26. Of course you might lose more or less than this amount, depending on your luck. For example, there is a 26.5% chance that you will be ahead after 100 single-dollar bets on red. This chance is figured by assuming that every possible string of 100 numbers is equally likely to appear on your 100 plays, and finding the proportion of these strings with more than 50 reds. That is quite a hefty calculation, not to be undertaken until Chapter 2. But it gives you an idea of how far the method of equally likely outcomes can be pushed. The argument just used to calculate the house percentage on red can be generalized to calculate the house percentage on any bet whatever. Consider a bet on A at payoff odds of r pay to 1 against. If you stake $1 on A, the house stakes $r pay, so the total at stake is $(rpay + 1). According to the fair odds rule, the fair price to pay would be proportion P(A) of the total stake, that is, $P(A) (rpay + 1) So out of your $1 bet, the fraction taken by the house is 1- P(A) (rpay + 1). That is to say House Percentage = [1 - P(A)(rpay + 1)] x 100%</p>
<p>For example, in a straight play at roulette, that is, a bet on a single number, the chance odds are 37 to 1 against, corresponding to a probability of 1 in 38. But the payoff odds are only 35 to 1 against. So for a straight play, the house percentage is 1 [1 - 38 (35 + 1)] x 100% = 5.26% the same as for bets on red or black. For single numbers there is a neat way of checking this house percentage. Imagine there are 38 gamblers, each of whom bets on a different number. Then the house collects $38 from each spin of the wheel. But one and only one of the gamblers wins each time. After each spin, the house pays off exactly $36, the winning gambler’S payoff of $35 plus $1 back. So the house collects $38 - $36 = $2 for sure from every spin. If this cost of $2 is thought of as shared equally among the 38 gamblers, the result is a cost of $2/38 = 5.26 cents per gambler. This is the house percentage. Over the long run, the different numbers come up about equally often. So each player would end up paying about that amount per game. Exercises 1. 1 1. In a certain population of adults there are twice as many men as women. What is the proportion of men in the population: a) as a fraction; b) as a percent; c) as a decimal? Repeat for a population in which there are four men to every three women. 2. Suppose a word is picked at random from this sentence. Find: a) the chance that the word has at least 4 letters; b) the chanc’e that the word contains at least 2 vowels Ca, e, i, 0, u); c) the chance that the word contains at least 4 letters and at least 2 vowels. 3. Sampling with and without replacement. Sampling with replacement: A box contains tickets marked 1,2, ... ,n.&nbsp;A ticket is drawn at random from the box. Then this ticket is replaced in the box and a second ticket is drawn at random. Find the probabilities of the following events: a) the first ticket drawn is number 1 and the second ticket is number 2; b) the numbers on the two tickets are consecutive integers, meaning the first numher drawn is one less than the second number drawn. c) the second number drawn is bigger than the first number drawn. Sampling without replacement: d) Repeat a) through c) assuming instead that the first ticket drawn is not replaced, so the second ticket drawn must be different from the first.</p>
<p>4. Suppose I bet on red at roulette and you bet on black, both bets on the same spin of the wheel. a) What is the probability that we both lose? b) What is the probability that at least one of us wins? c) What is the probability that at least one of us loses? 5. Suppose a deck of 52 cards is shuffled and the top two cards are dealt. a) How many ordered pairs of cards could possibly result as outcomes? Assuming each of these pairs has the same chance, calculate: b) the chance that the first card is an ace; c) the chance that the second card is an ace (explain your answer by a symmetry argument as well as by counting); d) the chance that both cards are aces; e) the chance of at least one ace among the two cards. 6. Repeat Exercise 5, supposing instead that after the first card is dealt, it is replaced, and shuffled into the deck before the second card is dealt. 7. Suppose two dice are rolled. Find the probabilities of the following events. a) the maximum of the two numbers rolled is less than or equal to 2; b) the maximum of the two numbers rolled is less than or equal to 3; c) the maximum of the two numbers rolled is exactly equal to 3. d) Repeat b) and c) for x instead cf 3, for each x from 1 to 6. e) Denote by P(x) the probability that the maximum number is exactly x. What should P(I) + P(2) + P(3) + P(4) + P(5) + P(6) equal? Check this for your answers to d). 8. Repeat Exercise 7 for two rolls of a fair n-sided die for an arbitrary n instead of 6. 9. The chance odds against an event occurring are 10 to 1. What is the chance of the event? What if the odds were 5 to 1 against? 10. Calculate the chance of a win and the house percentage for each of the bets at roulette described below the layout in Figure 1. 11. Show that if the fair (chance) odds against an event are rfair to 1, then in a bet at payoff odds of rpay to 1 the house percentage is rfair - rpay x 100% rfair + 1</p>
<p>1.2 Example 1. Section 1.2. Interpretations 11 Interpretations James Bernoulli (1654 - 1705), one of the founders of probability theory, put it like this: Probability is the degree of certainty, which is to the certainty as a part is to a whole. This conveys the right intuitive idea. And it points correctly to the rules of proportion as the mathematical basis for a theory of probability. But it leaves open the question of just how probabilities should be interpreted in applications. This section considers two important interpretations of probability. First, the frequency interpretation in which probabilities are understood as mathematically convenient approximations to long-run relative frequencies. Second, the subjective interpretation in which a probability statement expresses the opinion of some individual regarding how certain an event is to occur. Which (if either) of these interpretations is “right” is something which philosophers, scientists, and statisticians have argued bitterly for centuries. And very intelligent people still disagree. So don’t expect this to be resolved by the pre$ent discussion. Frequencies A relative frequency is a proportion measuring how often, or how frequently, something or other occurs in a sequence of observations. Think of some experiment or set of circumstances which can be repeated again and again, for example, tossing a coin, rolling a die, the birth of a child. Such a repeatable experiment may be called a trial. Let A be a possible result of such a trial: for example, the coin lands heads, the die shows a six, the child is a girl. If A happens m times in n trials, then min is the relative frequency of A in the n trials. Coin tossing. Suppose a coin is tossed ten times, and the observed sequence of outcomes is t, h, h, t, h, h, h, t, t, h, where each t indicates a tail and each h a head. The successive relative frequencies of heads in one toss, two tosses, and so on up to ten tosses are then 0122345556 1’2’3’4’5’6’7’8’9’ 10’ as graphed in Figure 1. Figure 2 shows what usually happens if you plot a similar graph of relative frequencies for a much longer series of trials. A general rule, illustrated in Figure 2, is that relative frequencies based on larger numbers of observations are less liable to fluctuation than those based on smaller</p>
<p>numbers. It is observed that almost regardless of the precise nature of the experimental trials in question, or what feature A of the trials is recorded, the relative frequency of A based on n trials tends to stabilize as n gets larger and larger, provided that the conditions of the trial are kept as constant as possible. This phenomenon is called the statistical regularity of relative frequencies, or the empirical law of averages. In coin tossing, heads and tails usually come up about equally often over a long series of tosses. So the long-run relative frequency of heads is usually close to 1/2. This is an empirical fact, closely linked to our intuitive idea that heads and tails are equally likely to come up on any particular toss. Logically, there is nothing to prevent the relative frequency of heads in a long series of tosses from being closer to, say, 1/4, or 2/3, than to 1/2. The relative frequency could even be 1 if the coin landed heads every time, or 0 if it landed tails every time. But while possible, it hardly ever happens that the relative frequency of heads in a long series of tosses differs greatly from 1/2. Intuitively, such a large fluctuation is extremely unlikely for a fair coin. And this is precisely what is predicted by the theory of repeated trials, taken up in Chapter 2. In the frequency interpretation, the probability of an event A is the expected or estimated relative frequency of A in a large number of trials. In symbols, the proportion of times A occurs in n trials, call it Pn(A), is expected to be roughly equal to the theoretical probability P(A) if n is large: Pn(A) ~ P(A) for large n Under ideal circumstances, the larger the number of trials n, the more likely it is that this approximation will achieve any desired degree of accuracy. This idea is</p>
<p>FIGURE 2. Relative frequencies of heads in two long series of coin tosses. For a small number of trials, the relative frequencies fluctuate quite noticeably as the number oi trials varies. But these fluctuations tend to decrease as the number of trials increases. Initially, the two sequences of relative frequencies look quite different. But after a while, both relative frequencies settle down around 1/2. (The two series were obtained using a computer random number generator to simulate coin tosses.)</p>
<p>made precise in Chapter 2 by a mathematical result called the law of large numbers. The theoretical probability P(A) may even be conceived theoretically as a limit of relative frequencies Pn(A) as n ---+ 00. While intuitively appealing, this idea can only be made precise in a theoretical framework allowing infinitely many trials, so it is not really practical. The practical point is that for large but finite values of n, say n = 1000 or 10,000, a theoretical probability P(A) may provide a useful approximation to a relative frequency Pn(A) based on n trials. Here are a few simple examples based on long-run frequencies. The first shows how the frequency interpretation dictates the right level of detail for an assumption of equally likely outcomes. Tossing two coins. Suppose a cup containing two similar coins is shaken, then turned upside down on a table. What is the chance that the two coins show heads? Consider the following solutions to this problem. Either they both show heads, or they don’t. These are the two possible outcomes. Assuming these are equally likely, the chance of both heads is 1/2. Regard the number of heads showing on the coins as the outcome. There could be 0 heads, 1 head, or 2 heads. Now there are three possible outcomes. Assuming these are equally likely, the chance of both heads is 1/3. Despite the fact that the coins are supposed to be similar, imagine that they are labeled in some way to distinguish them. Call one of them the first coin and the other the second. Now there are four outcomes which might be considered: hh: the first coin shows heads and the second coin shows heads; ht: the first coin shows heads and the second coin shows tails; th: the first coin shows tails and the second coin shows heads; and tt: the first coin shows tails and the second coin shows tails. Assume these four possible outcomes are equally likely. Then the event of both coins showing heads has a chance of 1/4. which of the solutions above is correct? So far as the formal theory is concerned, they all are! Each solution starts from a clearly stated assumption of equally likely outcomes, then correctly determines the probability based on this assumption. The assumptions are different, and the conclusions are different. So at most one of the solutions can be consistent with long-run frequencies. Which is the right one? The assumptions of Solution 1 are easily discredited. By the same reasoning as in that solution, the probability of two tails must also be 1/2. That leaves zero probability</p>
<p>for the event of a head and a tail, which is clearly ridiculous so far as long-run frequencies are concerned. Solution 2 looks quite plausible, and is not easy to fault by armchair reasoning. Solution 3 looks artificial in comparison. Why should it be necessary to distinguish between two similar coins? On balance, these arguments seem to point to the 1/3 of Solution 2 as the answer. But the reality check is the long-run frequency. As a matter of practical experiment, which you can try yourself, the long-run frequency turns out to be around 1/4, no matter whether you can distinguish between the coins or not. So Solution 3 is the one which matches up with long-run frequencies. There is a physical principle involved here, which is a useful guide for getting probabilities to match long-run frequencies. All macroscopic physical objects like coins, grains of sand, and so on, behave statistically as if they are distinguishable. So, if you want to calculate chances for rolling several dice or tossing several coins, you should always assume they are distinguishable when setting up the outcome space. Interestingly, however, physicists have found that atomic particles such as protons and electrons behave statistically as if they are genuinely indistinguishable. The moral of the above example is that even if an assumption of equally likely outcomes is appropriate at some level of description, this level is not something which can be judged on mathematical grounds alone. It must be judged using some further interpretation of probability, such as the long-run frequency idea. Furthermore, there are examples like tossing a biased coin, or recording the sex of a newborn child, where long-run frequencies seem to stabilize around some more or less arbitrary decimal fraction between 0 and 1. Sex of children. Table 1 shows that the relative frequency of boys among newborn children in the U.S.A. appears to be stable at around 0.513. Observation of the sex of a child is comparable to a scheme with equally likely outcomes obtained by drawing at random with replacement from a box of 1000 tickets, containing 487 tickets marked girl and 513 tickets marked boy. This allows probabilities for births to be calculated as if they were probabilities for random sampling from a box of tickets. But the analof,’Y is not complete. The individual tickets have no physical interpretation like the sides of a die or the pockets of a roulette wheel. And there seems to be no way to decide what the composition of the box should be without counting births. StilL the above data suggest a reasonable model for the outcome of a single birth: the outcome space {girl, boy}, with probability p = 0.513 for boy and 1 - p = 0.487 for girl.</p>
<p>’Births to residents of the U.S.A., based on 100% of births in selected states, and a 50% sample in all others. Source: Information Please Almanac, Atlas and Yearbook, 1985. Opinions The notion of probabilities as an approximation to long-run frequencies makes good sense in a context of repeated trials. But it does not always make sense to think in terms of repeated trials. Consider, for example: the probability of a panicular patient sUlViving an operation; the probability that a particular motorist is involved in an accident next year; the probability that a particular interest rate will be below 5% in a year’s time; the probability of a major earthquake in Berkeley before the year 2000. If you are the patient considering an operation, you want the doctor to tell you what he thinks your chances are. The notion of your undergoing repeated operations is absurd. Even if it is known that in similar operations in the past there was, say, a 10% fatality rate, this figure is irrelevant if the doctor knows that your state of health is much better, or you are much younger, or are different in some other respect from the population of patients on which the 10% figure is based. Rarely would it be possible for the doctor to know sUlVival percentages for patients just like you. The more factors that are taken into account, the more difficult it is to obtain relevant data, the smaller the number of cases on which figures could be based. If enough factors were taken into account, your case would be unique. What then are you to make of it if the doctor says you have a 95% chance of sUlViving? Essentially, this is a matter of opinion. In the doctor’s opinion, your chance of sUlVival is 95%. Another doctor might have another opinion, say 98%. You might ask several opinions, then somehow form your own opinion as to your chances.</p>
<p>Similar considerations apply to the other examples above. In none of these examples does the relative frequency idea make much sense. Ultimately, probability statements of this kind come down to some kind of intuitive judgment of the uncertainties involved. Such judgments lead to the notion of subjective probabilities, which may also be called probabilistic opinions, or degrees of belief This conception of probability corresponds well to everyday language, such as the following: It is unlikely that there will be an earthquake in Berkeley next year. If I toss a coin once, the probability that it will land heads is 1/2. The chance of rain tomorrow is 30%. Such statements have a superficial objective quality, since they make no reference to the person who is making them. But viewed as objective statements they are at best very hard to interpret, and at worst either meaningless or unverifiable. To give such statements meaning, it is simplest just to interpret them as expressions of probabilistic opinion. Intuitive comparison of probabilities can be helpful in formulating a probabilistic opinion. Comparisons can be made within a particular context, for example, by deciding that two or more events are equally likely, or that an event is twice as likely as another. Or comparisons can be made between different contexts. Comparison with a standard experiment like drawing tickets from a box can be a useful device. Which do you think is more likely? Event A, or getting a marked ticket on a draw at random from a box containing 20% marked tickets? If you think A is more likely, then you should assign a probability P(A) ~ 20%. If you have trouble deciding which is more likely, ask yourself which option you would prefer: To win a prize of some kind if A occurs, or to win the same prize if a marked ticket is drawn? Like the long-run frequency idea, the idea of subjective probability has its limitations. Subjective probabilities are necessarily rather imprecise. It may be difficult or impossible to pool the subjective probability opinions of different individuals about the same events. Assessment of subjective probabilities of events, regarded as having very small or very large probabilities, is very difficult, particularly if these events have important consequences for the person attempting to judge their probabilities. Despite such difficulties, the idea of interpreting probabilities as subjective opinions about uncertainties is something many people find reasonable. As well as broadening the range of application of probabilistic ideas, the subjective interpretation gives insight into the mathematics of probability theory. For example. the notion of conditional probability, introduced in the next chapter, captures the idea of how your probabilistic opinion may change over time as you acquire new information or data.</p>
<p>Exercises 1.2 1. If you get a speeding ticket in the state of Washington, it states on the ticket: “If you believe you did not commit the infraction, you may request a hearing. At the hearing, the state must prove by a preponderance of the evidence (more likely than not) that you committed the infraction.” What do you think the phrase “more likely than not” means? Does it refer to relative frequencies? to an opinion? if so, whose opinion? 2. If a bookmaker quotes payoff odds of 99 to 1 against a particular horse winning a race, does that suggest the chance that the horse will win is 1/100, less than 1/100, or more than 1/100? Explain. 3. Suppose there are 10 horses in a race and a bookmaker quotes odds of ri to 1 against horse i winning. Let Pi = ri~I’ i = 1 to 10, so each Pi is between 0 and 1. Let I; = PI + ... + PIO· a) Do you expect that I; is greater than, smaller than, or equal to l? Why? b) Suppose I; were less than 1. Could you take advantage of this? How? [Hint: By betting on all 10 horses in the race, a bettor can win a constant amount of money, regardless which horse wins.] 4. A gambler who makes 100 bets of SI, each at payoff odds of 8 to 1, wins 10 of these bets and loses 90. a) How many dollars has the gambler gained overall? b) What is the gambler’s average financial gain per bet? Suppose now that the gambler makes a sequence of $1 bets at payoff odds of rpay to 1. Define an empirical odds ratio r# to be the gambler’s number of losses divided by the number of wins. So, in the numerical example above, rpay was 8, and r # was 90/10 = 9. Show that the gambler’s average financial gain per bet is $(rpay - r #) I( r# + 1). Explain carefully the connection between this formula and the house percentage formula in Exercise 1.1.11.</p>
<p>1.3 Event language outcome space event impo ible event not A, oppo ire of A either A or B or both both A and B Section 1 .3. Distributions 19 Distributions From a purely mathematical point of view, probability is defined as a function of events. The events are represented as sets, and it is assumed that the probability function satisfies the basic rules of proportion. These are the rules for fractions or percentages in a population, and for relative areas of regions in a plane. To state the rules, we must first consider the representation of events as subsets of an outcome space. Suppose an outcome space n is given, and that all events of interest are represented as subsets of n.&nbsp;Think of n as representing all ways that some situation might turn out. It is no longer assumed that n is necessarily a finite set, or that all possible outcomes are equally likely. But if A is an event, the subset of n corresponding to A is still the set of all ways that A might happen. This subset of n will also be denoted A. Thus events are identified with subsets of n.&nbsp;TABLE 1. Translations between events and sets. To interpret the Venn diagrams in terms of events, imagine that a point is picked at random from the square. Each point in the square then represents an outcome, and each region of the diagram represents the event that the point is picked from that</p>
<p>The rules of probability involve logical relations between events. These are translated into corresponding relations between sets. For example, if C is the event which occurs if either A or B occurs (allowing the possibility that both A and B might occur), then the set of ways C can happen is tne union of the set of ways A can happen and the set of ways B can happen. In set notation, C = Au B. Table 1 gives a summary of such translations. Partitions Say that an event B is partitioned into n events B1, ... , Bn if B = B1 U B2 u· .. U Bn, and the events B1,“” Bn are mutually exclusive. That is to say, every outcome in B belongs to one and only one of the subsets Bi . Think of B as split up into separate cases B1, ... , Bn. Figure 1 shows a subset B of the square is partitioned in three different ways. However B is partitioned into subsets, or broken up into pieces, the area in B is the sum of the areas of the pieces. This is the addition rule for area. FIGURE 1. Partitions of a set B. B The addition rule is satisfied by other measures of sets instead of area, for example, length, volume, and the number or proportion of elements for finite sets. The addition rule now appears as one of the three basic rules of proportion. No matter how probabilities are interpreted, it is generally agreed they must satisfy the same three rules:</p>
<p>Rules of Proportion and Probability • Non-negative: P(B) 2: 0 • Addition: If B 1, B2 , ... ,Bn is a partition of B, then • Total one: p(n) = 1 A distribution over n is a function of subsets of n satisfying these rules. The term “distribution” is natural if you think of mass distributed over an area or volume n, and P(A) representing the proportion of the total mass in the subset A of n.&nbsp;Now think of probability as some kind of stuff, like mass, distributed over a space of outcomes. The rules for probability are very intuitive if you think informally of an event B as something that might or might not happen, and of P(B) as a measure of how likely it is that B will happen. It is agreed to measure probability on a scale of 0 to 1. The addition rule says that if something can happen in different ways, the probability that it happens is the sum of the probabilities of all the different ways it can happen. Technical remark. When the outcome space n is infinite, it is usually assumed that there is a similar addition rule for partitions of an event into an infinite sequence of events. See Section 3.4. In a rigorous treatment of measures like probability, length or area, defined as functions of subsets of an infinite set n, it is necessary to describe precisely those subsets of n, called measurable sets, whose measure can be unambiguously defined by starting from natural assumptions about the measure of simple sets like intervals or rectangles, using the addition rule, and taking limits. See Billingsley’s book Probability and Measure for details. Here are some useful general rules of probability. derived from the basic rules and illustrated by Venn diagrams. In the diagrams, think of probability as defined by relative areas. Complement Rule: The probability of the complement of A is P(not A) = P(AC) = 1 - P(A) Proof. Because n is partitioned into A and AC, and p(n) = 1, 0) Remarks. Note that if A = n, then AC = 0, the empty set, and P(A) = 1. So the rule of complements implies P(0) = O. The empty set contains nothing. Also, for a set A, P(A) = 1 - P(k) and P(AC) 2: 0, so P(A) .;:: 1. Thus probabilities are always between 0 and 1.</p>
<p>The next rule is a generalization of the rule of complements: Difference Rule: If occurrence of A implies occurrence of B, then P(A) :::; P(B), and the difference between these probabilities is the probability that B occurs and A does not: P(B and not A) = P(BAC) = P(B) - P(A) Proof. In other words, the assumption is that every outcome in A is an outcome in B, so A is a subset of B. Since B can be partitioned into A and (B but not A), P(B) = P(A) + P(BAC) by the addition rule. Now subtract P(A) from both sides. Inclusion-Exclusion: P(A U B) = P(A) + P(B) - P(AB) B Remarks. Here AuB means A or B or both (union) while AB means both A and B (intersection, A n B). This is the modification of the addition rule for events A and B that overlap, as in the following diagram. The addition rule for mutually exclusive A and B is the special case when AB = 0, so P(AB) = O. The extension to three or more sets is given in the exercises. Proof. As the diagram shows, the sets ABc, AB, and A C B form a partition of A U B, so P(A U B) = P(ABC) + P(AB) + P(AC B) Similarly P(A) = P(ABC) + P(AB) P(B) = P(ACB) + P(AB)</p>
<p>P(A) + P(B) = P(AB C ) + 2P(AB) + P(A C B) This is the same expression as for P(A U B), but P(AB) is included twice. Subtracting P(AB) excludes one of these terms, to give the inclusion-exclusion formula. Rich and famous. In a certain population, 10% of the people are rich, 5% are famous, and 3% are rich and famous. For a person picked at random from this population: What is the chance that the person is not rich? Here probabilities are defined by proportions in the population. By the rule of complements P(not rich) = 100% - P(rich) = 100% - 10% = 90% What is the chance that the person is rich but not famous? By the difference rule P(rich but not famous) = P(rich) - P(rich and famous) = 10% - 3% = 7% What is the chance that the person is either rich or famous? By the inclusion-exclusion formula, P(rich or famous) = P(rich) + P(famous) - P(rich and famous) = 10% + 5% - 3% = 12% Numbered tickets. Proportion P( i) of the tickets in a box are numbered i, with this distribution: number i 1 2 3 4 5 6 proportion P( i) 1/4 1/8 1/8 1/8 1/8 1/4 If a ticket is drawn at random from the box, what is the chance that the number on the ticket is 3 or greater?</p>
<p>Solution. Example 3. Problem. Solution. Assuming all tickets in the box are equally likely to be drawn, by the addition rule: 1 1 1 1 5 P(3 or 4 or 5 or 6) = P(3) + P(4) + P(5) + P(6) = “8 +”8 + “8 + 4 =”8 In the above example, outcomes with unequal probabilities (corresponding to various numbers) were obtained by partitioning a set of equally likely outcomes (the individual tickets) into subsets of different sizes. It was then possible to work with the probability distribution over the smaller number of outcomes defined by the partition, using the addition rule. This is the key to problems such as the following where there is no natural analysis in terms of equally likely outcomes. Shapes. A shape is a 6-sided die with faces cut as shown in the following diagram: t -- t t • ---- t - t • • • • t • • • • ~1’ • • ~ .. 1 ~ ~--1-----+ ~ +--1 -----+ The faces showing 1 and 6 are square faces with side of length one unit, but the distance between these faces, or the thickness of the shape, is a length t :S 1. So each of the faces 2, 3, 4, and 5 is a rectangle instead of a square. Such a die may land either flat (1 or 6), or on its side (2, 3, 4, or 5). As the thickness of the shape decreases from 1 to 0, it is intuitively clear that the chance that the shape lands flat increases continuously from 1/3 to 1. Suppose that the thickness t is such that the chance of the shape landing flat is 1/2. You could understand this to mean that over a long sequence of rolls, the shape landed flat about as often as it landed on its side. What is the probability that such a shape shows number 3 or greater? For i = 1 to 6, let P(i) be the probability that the shape lands showing i. Using the addition rule, 1/2 = P(flat) = P(l) + P(6) 1/2 = P(side) = P(2) + P(3) + P(4) + P(5) The symmetry of the shape suggests the assumptions: P(l) = P(6) and P(2) = P(3) = P(4) = P(5) These equations imply that the probabilities P( i) are as displayed in the following table and in Figure 2.</p>
<p>The probability that the shape shows a number greater than or equal to 3 is then given by the addition rule: 1 1 1 1 5 P(3 or 4 or 5 or 6) = P(3) + P(4) + P(5) + P(6) = “8 +”8 + “8 + 4 =”8 FIGURE 2. Histogram of the distribution in Example 3. This is a bar graph showing the probabilities for the shape showing face i. The area of the bar over i is proportional to P(i). By the addition rule for probabilities and areas, the probability that the shape shows a number greater than or equal to 3 is the shaded area relative to the total area, that is, 5/8. o o 1 2 3 4 5 6 Notice that the distribution of the number on the shape in Example 3 is identical to the distribution of a number drawn at random from the box of Example 2. The probability of getting a number greater than or equal to 3 is therefore the same in both examples. Similarly, for any subset B of {I, ... , 6}, the probability of getting an outcome in B is the same in both examples. The two procedures for obtaining a numerical outcome between 1 and 6, rolling the shape, and drawing a ticket from the box, are called probabilistically equivalent. In other words, the two outcomes have the same distribution. This means the set of possible outcomes and the distribution of probability over these outcomes is the same in both cases. It would not make sense, however, to say that the two procedures generated the same outcome. On the contrary, the two procedures would most likely produce two different numbers: Picking a number from a box and rolling a shape. Suppose one number is obtained by drawing at random from the box of tickets in Example 2, and another number is obtained by rolling a shape as in Example 3. What is the chance of the event that the number from the box is i and the number on the ticket is j? Consider the following two procedures for obtaining a pair of numbers (i,j): Draw from the box of tickets to obtain i. Roll the shape to obtain j. Draw from the box of tickets to obtain i. Replace this ticket in the box, mix up the tickets in the box and draw again to obtain j.</p>
<p>The second procedure is called random sampling with replacement (Exercise 1.1.3). It is intuitively clear that these two procedures must be probabilistic ally equivalent. That is to say the probability of any event determined by the first pair must be the same as the probability of the corresponding event for the second pair. In particular, the probability that the box produces i and the shape rolls j must be the same as the probability of getting i on the first draw and j on the second draw in two draws at random with replacement from the box. To solve the problem, let us assume this probabilistic equivalence. The point is that for two draws at random with replacement the probability of getting particular numbers i on the first draw and j on the second draw can be found by the method of Section 1.1. Suppose there are N tickets in the box, and that all N x N = N 2 possible pairs of tickets are equally likely in two draws at random with replacement. Since the number of tickets labeled i is P(i)N for P( i) displayed in Example 2, the number of ways to get (i, j) is P( i)N x P(j)N = P( i)P(j)N2 . So the required probability is P( i) P(j)N2 / N 2 = P( i)P(j). What is the probability that the two numbers are different? From the solution to the previous problem, for any particular number i, the probability of getting the same number i from the ticket and the shape is P(i)2.. Summing over i = 1, ... , 6 gives 6 P(ticket and shape show the same unspecified number) = L p(i)2 By the complement rule 6 P(ticket and shape show different numbers) = 1 - L p(i)2 ;=1 ;=1 13 16 The above example illustrates an important technique for solving probability problems. Look for a probabilistic equivalent of the original problem that is easier to understand. Then solve the equivalent problem. The solution of Problem 1 shows that the basic assumption made on intuitive grounds, that the ticket-shape scheme is probabilistically equivalent to a ticket- ticket scheme for draws with replacement implies a product rule for calculating the probability of an intersection of two events, one determined by the ticket and the other by the die: P(ticket shows number i and shape shows number j ) = P(ticket shows number i) P(shape shows number j )</p>
<p>for all i and j. Events A and B such as these, with P(AB) = P(A)P(B), are called independent events. The concept of independence is studied in Section 1.4. In language defined more formally in Section 3.1, the assumption of equivalence of the ticket-shape and ticket-ticket schemes can be restated as follows: the number on the ticket and the number rolled by the shape are independent random variables with the same distribution. Named Distributions The distribution on the set {I, ... , 6} defined by the probabilities P(l), ... , P(6) in the previous three examples is of no particular importance. It just illustrated numerically some general properties of a probability distribution over a finite set. There are some special distributions, however, that appear in a wide variety of contexts and are given names. Some of these named distributions are mentioned in the following paragraphs. Other named distributions appear throughout the book. There is a summary of the properties of the most important of these distributions on pages 476 to 488. Most named distributions have one or more parameters in their definition. These are constants appearing in the formula for the distribution which affect its shape and properties. Typically, the parameters are subject to some constraints such as non-negativity, so that the numbers defined by the formula satisfy the rules of probability. Bernoulli (p) distribution. For p between 0 and 1, this is the distribution on {O, I} defined by the following distribution table: possible outcome 0 1 probability 1-p p FIGURE 3. Histograms of some Bernoulli (p) distributions. rn 0 1 0 1 0 1 0 1 0 1 0 1 0 1 p=O p = 1/5 p = 1/3 p = 1/2 p = 2/3 p =4/5 p=l Think of any event A, for which you think it makes sense to consider the probability P(A). For example, A might be the event of heads on a coin toss, perhaps for a</p>
<p>biased coin. Now define an outcome to be 1 if A occurs, and 0 otherwise. If you like, imagine that you win a dollar if A occurs, nothing otherwise. Then the outcome is the number of dollars you win. This outcome, associated with the event A, is called the indicator of A. The distribution of the indicator of A is the Bernoulli (p) distribution for p = P(A). The number p is the parameter of the Bernoulli (p) distribution. The effect of increasing p from 0 to 1 is to shift the probability from being all concentrated at 0 to being all concentrated at 1, as shown by the histograms in Figure 3. Uniform distribution on a {’mite set. This distribution, defined by an assumption of equally likely outcomes, appeared in many examples in Section 1.1. To be clear about exactly what uniform distribution is meant, it is essential to define clearly the range of the uniform distribution, that is, the precise set of outcomes assumed equally likely. If the range is a set of n possible outcomes, for instance {1, 2 ... , n} or {O, 1, ... ,n - 1}, the probability of each possible outcome is l/n.&nbsp;The probability P(B) of an outcome in the set B is then P(B) = #(B)/n.&nbsp;Note that the uniform distribution on {O, I} is identical to the Bernoulli (1/2) distribution. This is the distribution of the indicator of heads on a fair coin toss. Uniform Ca, b) distribution. This refers to the distribution of a point picked uniformly at random from the interval (a, b) where a and b are two numbers with a &lt; b. The basic assumption is that probability is proportional to length. So for a &lt; x &lt; y &lt; b the probability that the point falls in the interval (x, y) is assumed to be (y - x)/(b - a). By rescaling the interval (a, b) to the unit interval (0,1), problems involving the uniform (a, b) distribution are reduced to problems involving the uniform (0,1) or standard uniform distribution. See Section 4.1 for details. Most calculators and computer languages have a command, often called “RND”, that produces a pseudo-random number with approximately uniform (0, 1) distribution. These numbers are called pseudo-random because the results of successive calls of RND are in fact generated by application of a simple deterministic formula starting from some initial number in (0,1), called the seed. The formula has the property that for 0 &lt; x &lt; y &lt; 1 the long-run relative frequency of numbers in (x, y) is almost exactly equal to y - x. By the addition rule for long-run frequencies, for any subset B of (0,1) which is a finite union of intervals, the long-run frequency with which RND generates numbers in B is almost exactly equal to the probability assigned to B by the uniform (0,1) distribution (that is the length of B, which is the sum of lengths of component intervals of B). Uniform distribution over an area in the plane. Now probabilities are defined by relative areas instead of relative lengths. Think of a point picked uniformly at random from the rectangular area in a Venn diagram. Long-run frequencies for pairs (RND1 , RND2) generated by two calls of a pseudo-random number generator are well approximated by probabilities derived from the uniform distribution on the unit square (0,1) x (0,1). Section 5.1 gives examples, and extensions of the idea to higher dimensions.</p>
<p>Empirical Distributions Let (XI,X2, ... ,xn ) be a list of n numbers. Think of Xi as the ith measurement of some physical quantity like the length or weight of something, in a series of repeated measurements. The empirical distribution of the list of n numbers is the distribution on the line (-00,00) defined by Pn(a, b) = #{i : 1 ::::; i ::::; n, a &lt; Xi &lt; b}/n That is, Pn (a, b) is the proportion of the n numbers in the list that lie in the interval (a, b). To give this distribution a probabilistic interpretation, imagine n tickets in a box with number Xi written on the ith ticket. Then for a ticket picked uniformly at random from the box, Pn (a, b) is the probability that the number on the ticket drawn is in (a, b). So the empirical distribution of a list is the distribution of a number picked at random from the list. The empirical distribution of a data list is displayed by a histogram. that is, a bar graph in which proportions in the list are represented by the areas of various bars. FIGURE 4. A data histogram. Actual values of the data points are shown by marks on the horizontal axis. The area of the bar over each bin shows the proportion of data points in the bin . .--- r- I 30 40 50 60 70 A histogram can be drawn as follows. First the interval of values is cut at some sequence of cut points bI &lt; b2 &lt; ... &lt; bm , such that all the Xi are contained in (bl , bm ), and none of the cut points equals any of the Xi. The cut points define m - 1 subintervals (bj , bj+1), 1 ::::; j ::::; m-l, called bins. The histogram is drawn by placing a rectangle over the jth bin with base the bin width bj+1 - bj and height Pn(bj , bj +1) _ #{i : 1 ::::; i ::::; n, bj &lt; Xi &lt; bj+d (bj +1 - bj ) - n(bj+1 - bj ) This height is the proportion of observations per unit length in the jth bin. The area of the bar over the jth bin is the base times height, which is the proportion of</p>
<p>observations in the jth bin: The total area of the histogram is the sum of the areas of these bars, which is m-l L Pn(bj , bj+d = Pn(b1 , bm) = 1 j=l by the addition rule for proportions, and the choice of b1 and bm so that all the observations lie in (b1 , bm ). A histogram smoothes out the data to display the general shape of an empirical distribution. Such a histogram often follows quite a smooth curve. This leads to the idea, developed in Section 4.1, of approximating empirical proportions by areas under a curve. The same idea is used in Section 2.2 to approximate probability histograms. Exercises 1.3 1. Suppose a cake is divided into three portions, one for you, one for your friend, and one for your neighbor. If you get twice as much as your friend, and your friend gets twice as much as your neighbor, what proportion of the cake do you get? 2. Write down the expression in set notation corresponding to each of the following events. a) the event which occurs if exactly one of the events A and B occurs; b) the event which occurs if none of the events A, B, or C occurs; c) the events obtained by replacing “none” in b) by “exactly one,” “exactly two,” and “three.” 3. Five hundred tickets, marked 1 through 500, are sold at a high-school cake raffle. I have tickets 17, 93, and 202. My friend has tickets 4, 101, 102, and 398. One of the tickets will be chosen at random, and the owner of the winning ticket gets a cake. Make an outcome space for this situation, and indicate how each of the following events can be represented as a subset of your outcome space. a) one of my tickets is the winner; b) neither my friend nor I win the raffle; c) the number on the winning ticket is just 1 away from the number on one of my tickets. 4. Let n = {O, 1, 2} be the outcome space in a model for tossing a coin twice and observing the total number of heads. Say if the following events can be represented as subsets of n.&nbsp;If you say “yes,” provide the subset; if you say “no,” explain why: a) the coin does not land heads both times; b) on one of the tosses the coin lands heads, and on the other toss it lands tails;</p>
<p>c) on the first toss the coin lands heads, and on the second toss it lands tails; d) the coin lands heads at least once. 5. Think of the set fl = {H H H, H HT, HT H, HTT, T H H, T HT, TT H, TTT} as the outcome space for three tosses of a coin. For example, the subset {H H H, TTT} corresponds to the event that all three tosses land the same way. Give similar verbal descriptions for the events described by each of the following subsets of fl.&nbsp;a) {HHH,HHT,HTH,HTT} b) {HTH,HTT,TTT,TTH} c) {HTT,HTH,HHT,HHH} d) {HHH,HHT,HTH,THH} e) {THT,HTT,TTH} f) {HHT,HHH,TTH,TTT} 6. Suppose a word is picked at random from this sentence. a) What is the distribution of the length of the word picked’ b) What is the distribution of the number of vowels in the word’ 7. Shapes. Following Example 3, suppose the probability that the shape lands flat (1 or 6) is p for some 0 :S p :S 1. a) For each k = 1,2, ... ,6 find a formula for P(k) in terms of p.&nbsp;b) Find a formula in terms of p for the probability that the number shown by the shape is 3 or more. 8. Let A and B be events such that P(A) = 0.6, P(B) = 0.4, and P(AB) = 0.2. Find the probabilities of: a) Au B b~ AC c) W d) AC B e) Au B C f) AC Be 9. Events F, G, and H are such that P(F) = 0.7, P(G) = 0.6, P(H) = 0.5, P(FG) = 0.4, P(FH) = 0.3, P(GH) = 0.2, P(FGH) = 0.1. Find: (a) P(F U G); (b) P(F U G U H); (c) P(FcGc H). 10. Events A, B, and C are defined in an outcome space. Find expressions for the following probabilities in terms of P(A), P(B), P(C), P(AB), P(AC), P(BC), and P(ABC). a) The probability that exactly two of A, B, C occur. b) The probability that exactly one of these events occurs. c) The probability that none of these events occur. 11. Inclusion-exclusion formula for 3 events. Write AUBuC = (AUB)UC and use the inclusion-exclusion formula three times to derive the inclusion-exclusion formula for 3 events: P(AUBUC) = P(A) + P(B) + P(C) - P(AB) - P(AC) - P(BC) + P(ABC) 12. Inclusion-exclusion formula for n events. Derive the inclusion-exclusion formula for n events i:::::: 1 i</p>
<p>by mathematical induction after showing that 13. Boole’s inequality. The inclusion-exclusion formula gives the probability of a union of events in terms of probabilities of intersections of the various subcollections of these events. Because this expression is rather complicated, and probabilities of intersections may be unknown or hard to compute, it is useful to know that there are simple bounds. Use induction on n to derive Boole’s inequality: P(U~=l A,) S; 2:~=1 P(Ai). 14. Show that P(A n B) ~ P(A) + P(B) - 1. 15. Use Boole’s inequality and the fact that (U~=l A;)C = n~=l Ai to show that n P(B1B2 .•. Bn) ~ L P(Bi) - (n - 1) i=l 16. Bonferroni’s inequalities. According to Boole’s inequality, the first sum in the inclusion -exclusion formula gives an upper bound on the probability of a union. This is the first of the series of Bonfer-rani inequalities. The next shows that the first sum minus the second is a lower bound. Show by using induction on n, and Boole’s inequality, that: a) P(Ui’=lA;) ~ 2:~=1 P(Ad - 2:;</p>
<p>1.4 Example 1. Section 1.4. Conditional Probability and Independence 33 Conditional Probability and Independence The first few examples of this section illustrate the idea of conditional probability in a setting of equally likely outcomes. Three coin tosses. If you bet that 2 or more heads will appear in 3 tosses of a fair coin, you are more likely to win the bet given the first toss lands heads than given the first toss lands tails. To be precise, assume the 8 possible patterns of heads and tails in the three tosses, {hhh, hht, hth, htt, thh, tht, tth, ttt}, are equally likely. Then the overall or unconditional probability of the event A = (2 or more heads in 3 tosses) = {hhh, hht, hth, thh} is P(A) = 4/8 = 1/2. But given that the first toss lands heads (say H), event A occurs if there is at least one head in the next two tosses, with a chance of 3/4. So it is said that the conditional probability of A given H is 3/4. The mathematical notation for the conditional probability of A given H is P(AIH), read “P of A given H”. In the present example P(AIH) = 3/4 because H = {hhh, hht, hth, htt} can occur in 4 ways, and just 3 of these outcomes make A occur. These 3 outcomes define the event {hhh, hht, hth} which is the intersection of the events A and H, denoted A and H, An H, or simply AH. Similarly, if the event He = “first toss lands tails” occurs, event A happens only if the next two tosses land heads, with probability 1/4. So Conditional probabilities can be defined as follows in any setting with equally likely outcomes. Counting Formula for P(A 18) For a finite set n of equally likely outcomes, and events A and B represented by subsets n, the conditional probability of A given B is P(AIB) = #(AB) #(B) the proportion of outcomes in B that are also in A. Here AB = A n B = A and B is the intersection of A and B.</p>
<p>Example 2. Problem. Solution. Example 3. Problem. Solution. Tickets. A box contains 10 capsules, similar except that four are black and six are white. Inside each capsule is a ticket marked either win or lose. The capsules are opaque, so the result on the ticket inside cannot be read without breaking open the capsule. Suppose a capsule is drawn at random from the box, then broken open to read the result. If it says win, you win a prize. Otherwise, you win nothing. The numbers of winning and losing tickets of each color are given in the diagram, which shows the tickets inside the capsules. Suppose that the capsule has just been drawn, but not yet broken to read the result. The capsule is black. Now what is the probability that you win a prize? (~) @ Q~v Q~v @ @ C§V @ @ @ This conditional probability is the proportion of winners among black capsules: P( . Ibl k) = #(win and black) = ~ = 0 5 Win ac #(black) 4’ Compare with the unconditional probability P(win) = 4/10 = 0.4 Two-sided cards. A hat contains three cards. One card is black on both sides. One card is white on both sides. One card is black on one side and white on the other. The cards are mixed up in the hat. Then a single card is drawn and placed on a table. If the visible side of the card is black, what is the chance that the other side is white? Label the faces of the cards: bl and b2 for the black-black card; WI and W2 for the white-white card; b3 and W3 for the black-white card. bib b/w w/w Assume that each of these six faces is equally likely to be the face showing uppermost. Experience shows that this assumption does correspond to long-run frequencies, provided the cards are similar in size and shape, and well mixed up in</p>
<p>the hat. The outcome space is then the set of six possible faces which might show uppermost: The event {black on top} is identified as Similarly, Given that the event {black on top} has occurred, the face showing is equally likely to be b1, b2 , or b3 . Only in the last case is the card white on the bottom. So the chance of white on bottom given black on top is P(white on bottomlblack on top) #(white on bottom and black on top) 1 #(black on top) 3 You might reason as follows: The card must be either the black-black card or the black-white card. These are equally likely possibilities, so the chance that the other side is white is 1/2. Many people find this argument convincing, but it is basically wrong. The assumption of equally likely outcomes, given the top side is black, is not consistent with long-run frequencies. If you repeat the experiment of drawing from the hat over and over, replacing the cards and mixing them up each time, you will find that over the long run, among draws when the top side is black, the bottom side will be white only about 1/3 of the time, rather than 1/2 of the time. Frequency interpretation of conditional probability. This is illustrated by the previous example. If P(A) approximates to the relative frequency of A in a long series of trials, then P(AIB) approximates the relative frequency of trials producing A among those trials which happen to result in B. A general formula for P(AIB), consistent with this interpretation, is found as follows. Start with the counting formula for P(AIB) in a setting of equally likely outcomes, then divide both numerator and denominator by #(f!) to express P(AIB) in terms of the unconditional probabilities P(AB) = #(AB)/#(fl) and P(B) = #(B)/#(f!): P(AIB) = #(AB) #(B) #(AB)/#(f!) #(B)/#(f!) P(AB) P(B)</p>
<p>General Formula for P(A 18) P(AIB) = P(AB) P(B) If probabilities P(A) are specified for subsets A of an outcome space n, then conditional probabilities given B can be calculated using this formula. This restricts the outcome space to Band renormalizes the distribution on B. In case the original distribution is defined by relative numbers, or relative areas, the same will be true of the conditional distribution given B, but with the restriction from n to B. To make a clear distinction, P(A) or P(AB) is called an overall or unconditional probability, and P(AIB) a conditional probability. Relative areas. Suppose a point is picked uniformly at random from the big rectangle in the diagram. Imagine that information about the position of this point is revealed to you in two stages, by the answers to the following questions: Question 1. Is the point inside the circle B? Question 2. Is the point inside the rectangle A? If the answer to Question 1 is yes, what is the probability that the answer to Question 2 will be yes? The problem is to find the probability that the point is in the rectangle A given that it is in the circle B. By inspection of the diagram, approximately half the area inside B is inside A. So the required probability is P(AIB) = P(AB) = Area(AB) ~ 1/2 P(B) Area(B) The formula for conditional probability in this case corresponds to the idea that given the point is in B, equal areas within B still have equal probabilities. Tree Diagrams and the Multiplication Rule In the above example a conditional probability was calculated from overall probabilities. But in applications there are usually many events A and B such that the conditional probability P(AIB) and the overall probability P(B) are more obvious than the overall probability P(AB). Then P(AB) is calculated using the following rearrangement of the general formula for conditional probability:</p>
<p>Multiplication Rule P(AB) = P(AIB)P(B) This rule is very intuitive in terms of the frequency interpretation. If, for example, B happens over the long run about 1/2 the time (P(B) = 1/2), and about 1/3 of the times that B happens A happens too (P(AIB) = 1/3), then A and B happens about 1/3 of 1/2 = 1/3 x 1/2 = 1/6 of the time (P(AB) = P(AIB)P(B) = 1/6). The multiplication rule is often used to set up a probability model with intuitively prescribed conditional probabilities. Typically, A will be an event determined by some overall outcome which can be thought of as occurring by stages, and B will be some event depending just on the first stage. If you think of B happening before A it is more natural to rewrite the multiplication rule, with BA instead of AB and the two factors switched: P(BA) = P(B)P(AIB) In words, the chance of B followed by A is the chance of B times the chance of A given B. Picking a box, then a ball. Suppose that there are two boxes, labeled odd and even. The odd box contains three balls numbered 1, 3, 5. The even box contains two balls labeled 2, 4. One of the boxes is picked at random by tossing a fair coin. Then a ball is picked at random from this box. What is the probability that the ball drawn is ball 3? A scheme like this can be represented in a tree diagram. Each branch represents a possible way things might turn out. Probabilities and conditional probabilities are indicated along the branch.</p>
<p>Because the box is chosen by a fair coin toss, P(odd) = P(even) = 1/2 The only way to get 3 is to first pick the odd box, then pick 3. By assumption P(3Iodd) = 1/3 Now by the multiplication rule, 1 1 1 P(3) = P( odd and 3) = P( odd)P(31 odd) = ’2 x 3 = ’6 This is the product of the probabilities along the path representing the outcome 3. The corresponding products along the other possible branches give the distribution displayed in the tree diagram. This is a different representation of the same problem, using a Venn diagram. ODD 1 EVEN 3 5 2 4 A naive approach to the above problem would be to assume that all outcomes were equally likely. But this would imply P(first box) = P(odd) = 3/5 P(second box) = P(even) = 2/5 which is inconsistent with the box being chosen by a fair coin toss. The problem could also be solved without conditional probabilities by a symmetry argument, assuming that P(l) = P(3) = P(5) and P(2) = P(4) P(l) + P(3) + P(5) = P(2) + P(4) = 1/2 These equations yield the same answer as above.</p>
<p>To summarize the method of the previous example: Multiplication Rule in a Tree Diagram After setting up a tree diagram whose paths represent joint outcomes, the multiplication rule is used to define a distribution of probability over paths. The probability of each joint outcome represented by a path is obtained by multiplying the probability and conditional probability along the path. Electrical components. Suppose there are two electrical components. The chance that the first component fails is 10%. If the first component fails, the chance that the second component fails is 20%. But if the first component works, the chance that the second component fails is 5%. Calculate the probabilities of the following events: 1. at least one of the components works; 2. exactly one of the components works; 3. the second component works. Here is the tree diagram showing all possible performances of the first and second components. Probabilities are filled in using the above data and the rule of complements.</p>
<p>By inspection of the diagram, P(at least one works) = 1 - P(both fail) = 1 - 0.1 x 0.2 = 0.98 P(exactly one works) = P(first works and second fails) + P(first fails and second works) = 0.9 x 0.05 + 0.1 x 0.8 = 0.125 P(second works) = P(first works and second works) + P(first fails and second works) = 0.9 x 0.95 + 0.1 x 0.8 = 0.935 Averaging Conditional Probabilities The last two parts of the previous example illustrate a rule of average conditional probabilities: for any events A and B, the overall probability P(A) is the average of the two conditional probabilities P(AIB) and P(AIBC) with weights P(B) and P(B C ): In the example, Band B C were (first works) and (first fails), while A was (exactly one works) in one instance, and (second works) in the other. The formula gives the probability of A as the sum of products of probabilities along paths leading to A in the tree diagram. The event B defines a partition of the whole outcome space n into two events Band BC, corresponding to two initial branches in the tree. There is a similar formula for any partition B1 , ... , Bn of the whole outcome space n, corresponding to n initial branches of a tree. For any event A the events AB1 , ... , ABn form a partition of A, so P(A) = P(ABd + ... + P(ABn) by the addition rule. Applying the multiplication rule to each term gives This important result is summarized in the following box.</p>
<p>Rule of Average Conditional Probabilities For a partition B1 , ... ,Bn of fl, In words: the overall probability P(A) is the weighted average of the conditional probabilities P(AIBi) with weights P(Bi). Sampling without replacement. Suppose two cards are dealt from a well-shuffled deck of 52 cards. What is the probability that the second card is black? A common response to this question is that you can’t say. It depends on whether the first card is black or not. If the first card is black, the chance that the second is black is 25/51, since no matter which black card the first one is, the second is equally likely to be any of the 51 remaining cards, and there are 25 black cards remaining. If the first card is red, the chance that the second is black is 26/51, by similar reasoning. These are the conditional probabilities of black on the second card given black and red, respectively, on the first card. But the question does not refer to the first card at all. The overall probability of black on the second card is the average of these conditional probabilities: P(second black) = P(second blacklfirst black)P(first black) + P(second blacklfirst red)P(first red) = 25 . ~ 26. ~ = (25 + 26) x ~ = ~ 51 2 + 51 2 51 2 2 This can also be argued by symmetry. Since there are equal numbers of black and red cards in the deck, the assumptions made at the start are symmetric with respect to black and red. This makes P(second black) = P(second red) Since P(second black) + P(second red) = 1 this gives the answer of 1/2. This argument shows just as well that if n cards are dealt, then P(nth card black) = 1/2, P(nth card an ace) = 1/13, and so on.</p>
<p>Independence We have just seen that for any events A and B, P( A) is the average of the conditional probabilities P(AIB) and P(AIBC), weighted by P(B) and P(BC). Suppose now that the chance of A does not depend on whether or not B occurs, and in either case equals p, say. In symbols: P(AIB) = P(AIBC) = p (1) Then also the unconditional probability of A is p: P(A) = P(AIB)P(B) + P(AIBC)P(BC) = pP(B) + pP(BC) = p For example, A might be the event that a card dealt from a well-shuffled deck was an ace, B the event that a die showed a six. Such events A and B are called independent. Intuitively, independent events have no influence on each other. It would be reasonable to suppose that any event determined by a card dealt from a shuffled deck would be independent of any event determined by rolling a die. To be brief, the deal and the die roll would be called independent. One more example: two draws at random from a population would be independent if done with replacement between draws, but dependent (i.e., not independent) if done without replacement. Independence of events A and B can be presented mathematically in a variety of equivalent ways. For example, it was just shown that the definition (1) above (which assumes both P(B) &gt; 0 and P(BC) &gt; 0), implies P(AIB) = P(A) (2) A similar calculation shows that (2) implies (1). The formula P(AIB) = P(AB)/ P(B) shows (2) is equivalent to the following: Multiplication Rule for Independent Events P(AB) = P(A)P(B) The multiplication rule is usually taken as the formal mathematical definition of independence, to include the case of events with probability 0 or 1. (Such an event is then, by definition, independent of every other event.) The multiplication rule brings out the symmetry of independence. Assuming P(A) &gt; 0, and using the fact that AB = BA and P(A)P(B) = P(B)P(A), the multiplication rule allows (2) to be turned around to P(BIA) = P(B)</p>
<p>and (1) can be turned around similarly. Assuming A and B are independent, all of these formulae hold also with either AC substituted for A, B C for B, or with both substitutions. This is obvious for (1), hence also true for the others. To spell out an example, since A splits into ABc and AB, P(ABC ) = P(A) - P(AB) = P(A)-P(A)P(B) assuming the multiplication rule for A and B = P(A)(l - P(B)) = P(A)P(BC ) by the rule of complements. So the multiplication rule works just as well with BC instead of B. The same goes for AC instead of A. Here the various probabilities determined by inde- I pendent events A and B are illustrated graphically A C as proportions in a Venn diagram. Event A is represented by a rectangle lying horizontally, event B by ;:.f-------+----1 a rectangle standing vertically. A Reliability of two components in series. A system consists of two components C1 and C2 , each of which must remain operative for the overall system to function. The components C1 and C2 are then said to be connected in series, and represented diagrammatically as follows: Let Wi be the event that component Ci works without failure for a given period of time, say one day. The event that the whole system operates without failure for one day is the event that both C1 and C2 operate without failure, that is, the event WI W2 . The probabilities P(Wd and P(W2 ) are called the reliabilities of components C1 and C2 . The probability P(WI W2 ) is the reliahility of the whole system. Suppose that the component reliabilities P(Wd and P(W2 ) are known from empirical data of past performances of similar components, say P(Wd = 0.9 and P(W2 ) = 0.8. If the particular components C1 and C2 have never heen used together before, P(WI W2 ) cannot be known empirically. But it may still be reasonable to assume that the events WI and W2 are independent. Then the reliability of the whole system would be given hy the formula P(system works) = P(WI W2 ) = P(WdP(W2 ) = 0.9 x 0.8 = 0.72</p>
<p>Hopefully this number, 0.72, would give an indication of the long-run relative frequency of satisfactory performance of the system. But bear in mind that such a number is based on a theoretical assumption of independence which mayor may not prove well founded in practice. The sort of thing which might prevent independence is the possibility of failures of both components due to a common cause, for example, voltage fluctuations in a power supply, the whole system being flooded, the system catching fire, etc. For the series system considered here such factors would tend to make the reliability P(WI W2 ) greater than if WI and W2 were independent, suggesting that the number, 0.72, would be too Iowan estimate of the reliability. Reliability of two components in parallel. A method of increasing the reliability of a system is to put components in parallel, so the system will work if either of the components works. Two components C1 and C2 in parallel may be represented diagrammatically as follows: Suppose, as in the last example, that the individual components C1 and C2 have reliabilities P(Wd and P(W2)’ where WI is the event that C1 works. The event that the whole system functions is now the event WI U W2 that either C1 or C2 works. The complementary event of system failure is the event FIF2 that both C1 and C2 fail, where Fi is the complement of Wi. Thus the reliability of the whole system is If WI and W2 are assumed independent, so are Fl and F2 . In that case P(system works) = 1 - P(F1 )P(F2) For example, if the component reliabilities are P(Wd = 0.9 and P(W2 ) = 0.8 as before, then P(F1 ) = 0.1 and P(F2) = 0.2, and the system reliability is P(system works) = 1 - (0.1)(0.2) = 0.98 This is a considerable improvement over the reliability of the individual components. The assumption of independent failures must be viewed with particular suspicion in parallel systems, as it tends to lead to exaggerated estimates of system reliabilities. Suppose, for example, that all failures of component C1 and half the failures of component C2 occur due to severe voltage fluctuation in a power supply common to C1 and C2 . Then Fl is the event of a voltage fluctuation, and it should be assumed</p>
<p>that P(F1 1F2) = 0.5 instead of the independence assumption P(FIIF2) = 0.1. With the new assumptions, P(system works) = 1 - P(F1F2) = 0.9 As a general rule, failures of both components due to a common cause will tend to decrease the reliability of a parallel system below the value predicted by an independence assumption. Exercises 1.4 1. In a particular population of men and women, 92% of women are right handed, and 88% of men are right handed, Indicate whether each of the following statements is (i) true. (ij) false, or (iii) can’t be decided on the basis of the information given. a) The overall proportion of right handers in the population is exactly 90%. b) The overall proportion of right handers in the population is between 88% and 92%. c) If the sex ratio in the population is 1-to-1 then a) is true. d) If a) is true then the sex ratio in the population is 1-to-1. e) If there are at least three times as many women as men in the population, then the overall population of right handers is at least 91%. 2. A light bulb company has factories in two cities. The factory in city A produces twothirds of the company’s light bulbs. The remainder are produced in city B, and of these, 1% are defective. Among all bulbs manufactured by the company, what proportion are not defective and made in city B? 3. Suppose: P(rain today)=40%; P(rain tomorrow) =50%; P(rain today and tomorrow)=30%. Given that it rains today, what is the chance that it will rain tomorrow? 4. Two independent events have probabilities 0.1 and 0.3. What is the probability that a) neither of the events occurs? b) at least one of the events occurs? c) exactly one of the events occurs? 5. There are two urns. The first urn contains 2 black balls and 3 white balls. The second urn contains 4 black balls and 3 white balls. An urn is chosen at random, and a ball is chosen at random from that urn. a) Draw a suitable tree diagram. b) Assign probabilities and conditional probabilities to the branches of the tree. c) Calculate the probability that the ball drawn is black</p>
<p>6. Suppose two cards are dealt from a deck of 52. What is the probability that the second card is a spade given that the first card is black? 7. Suppose A and B are two events with P(A) = 0.5, P(A U B) = O.S. a) For what value of P(B) would A and B be mutually exclusive? b) For what value of P(B) would A and B be independent? 8. A hat contains a number of cards, with 30% white on both sides; 50% black on one side and white on the other; 20% black on both sides. The cards are mixed up, then a single card is drawn at random and placed on the table. If the top side is black, what is the chance that the other side is white? 9. Three high schools have senior classes of size 100, 400, and 500, respectively. Here are two schemes for selecting a student from among the three senior classes: A: Make a list of all 1000 seniors, and choose a student at random from this list. B: Pick one school at random, then pick a student at random from the senior class in that school. Show that these two schemes are not probabilistic ally equivalent. Here is a third scheme: C: Pick school i with probability Pi (PI +P2 +P3 = 1), then pick a student at random from the senior class in that school. Find the probabilities PI, P2, and P3 which make scheme C equivalent to scheme A. 10. Suppose electric power is supplied from two independent sources which work with probabilities 0.4, 0.5, respectively. If both sources are providing power enough power will be available with probability 1. If exactly one of them works there will be enough power with probability 0.6. Of course, if none of them works the probability that there will be sufficient supply is O. a) What are the probabilities that exactly k sources work for k = 0, 1, 2? b) Compute the probability that enough power will be available. 11. Assume identical twins are always of the same sex, equally likely boys or girls. Assume that for fraternal twins the firstborn is equally likely to be a boy or a girl, and so is the secondborn, independently of the first. Assume that proportion P of twins are identical, proportion q = 1-P fraternal. Find formulae in terms of P for the following probabilities for twins: a) PCboth boys) b) PCfirstborn boy and secondborn girl) c) PCsecondborn girl I firstborn boy) d) PCsecondborn girl I firstborn girl). 12. Give a formula for P(FIGC ) in terms of P(F), P(G), and P(FG) only.</p>
<p>1.5 Example 1. Problem. Solution. Section 1.5. Bayes’ Rule 47 Bayes’ Rule The rules of conditional probability, described in the last section, combine to give a general formula for updating probabilities called Bayes’ rule. Before stating the rule in general, here is an example to illustrate the basic setup. Which box? Suppose there are three similar boxes. Box i contains i white balls and one black ball, i = 1,2,3, as shown in the following diagram. loelloil Box 1 Box 2 1001 ~ Box 3 Suppose I mix up the boxes and then pick one at random. Then I pick a ball at random from the box and show you the ball. I offer you a prize if you can guess correctly what box it came from. Which box would you guess if the ball drawn is white and what is your chance of guessing right? An intuitively reasonable guess is Box 3, because the most likely explanation of how a white ball was drawn is that it came from a box with a large proportion of whites. To confirm this, here is a calculation of P( ‘1 h’) P(Box i and white) Box t w He = ( ) P white (i = 1,2,3) These are the chances that you would be right if you guessed Box i, given that the ball drawn is white. The following diagram shows the probabilistic assumptions:</p>
<p>From the diagram, the numerator in (*) is 1 i P(Box i and white) = P(Box i)P(whiteIBox i) = 3” x i + 1 (i = 1,2,3) By the addition rule, the denominator in (*) is the sum of these terms over i = 1, 2, 3: 1 1 1 2 1 3 23 P(white) = 3” x 2” + 3” x 3” + 3” x 4 = 36 and 1 i 12 ,; 3 x HI • P(Box ilwhite) = 23 = - X -- 36 23 i + 1 (i=1,2,3) Substituting for i/(i + 1) for i = 1,2,3 gives the following numerical results: i 1 2 3 P(Box ilwhite) 6/23 8/23 9/23 This confirms the intuitive idea that Box 3 is the most likely explanation of a white ball. Given a white ball, the chance that you would be right if you guessed this box would be 9/23 ~ 39.13%. Suppose, more generally, that events Bl , ... , Bn represent n mutually exclusive possible results of the first stage of some procedure. Which one of these results has occurred is assumed unknown. Rather, the result A of some second stage has been observed, whose chances depend on which of the Bi’S has occurred. In the previous example A was the event that a white ball was drawn and Bi the event that it came from a box with i white balls. The general problem is to calculate the probabilities of the events Bi given occurrence of A (called posterior probabilities), in terms of co the unconditional probabilities P(Bi ) (called prior probabilities); (ii) the conditional probabilities P(AIBi) (called likelihoods). Here is the general calculation: (multiplication rule) where, by the rule of average conditional probabilities, the denominator is P(A) = P(AIBdP(Bd + ... + P(AIBn)P(Bn) which is the sum over i = 1 to n of the expression P(AIBi)P(Bi ) in the numerator. The result of this calculation is called Bayes’ rule.</p>
<p>Bayes’ Rule For a partition B1 , ... ,Bn of all possible outcomes, It is better not to try to memorize this formula, as it is easily derived from the basic rules of conditional probability which are easier to remember. Rather, understand the sequence of steps by which it is derived. These are the same steps used to solve the balls and boxes problem. Which box? (continued). Consider again the same three boxes as in the previous example. Suppose I pick a box. Then I pick a ball at random from the box and show you the ball. I offer you a prize if you can guess correctly what box it came from. Which box would you guess if the drawn ball is white, and what is your chance of guessing right? The wording of this problem is identical to the wording of Example 1 above, except that the sentence “Suppose I mix up the boxes and then pick one at random” has been replaced by “Suppose I pick a box”. A naive approach to the new problem is to suppose it is the same as the old one, with the answer: guess Box 3, with probability of being right = 9/23 But this makes an implicit assumption that I am equally likely to pick anyone of the three boxes. And the problem cannot be solved without assuming some values 1ri for the probabilities that I pick box i, i = 1,2,3. These probabilities 1ri are called prior probabilities because they refer to your opinion about which box I picked, prior to learning the color of the ball drawn. Once you have assigned these prior probabilities 1ri, i = 1,2,3, the previous calculations can be repeated. From the prior probabilities 1ri and the probabilities i / (i + 1) of getting the observed result, given box i (the likelihoods), you can obtain the posterior probabilities by Bayes’ rule: 1ri (~l) P(Box ilwhite) = 1 ‘2 3 1rl X “2 + 1r2 X 3” + 1r3 X 4” Thus, given that a white ball was drawn, to maximize your chance of guessing correctly you should guess box i for whichever i maximizes 1ri (i~l)’ Which i this is depends on the 1ri. The probabilities in question are now clearly a matter of your opinion about how I picked the box. There remains the problem of how to assign the prior probabilities 1ri. This is a tricky business, as it depends on psychological</p>
<p>factors, such as whether or not you think I am deliberately trying to make it hard for you to guess, and if so what strategy you think I’m using. For further analysis, see Exercises 1.5.7 and 1.5.8. In principle, every application of Bayes’ rule is such as the above examples of guessing the box that produced a particular color of ball. There is always the problem of deciding what the prior probabilities should be. Most often the prior probabilities will only make sense in a subjective interpretation of probability. But in problems like the next example (false positives) the prior probabilities may be known as population proportions. This example is like a scheme with two boxes D and DC: Box D containing 95% balls labeled + and 5% labeled - Box DC containing 2% balls labeled + and 98% labeled - If box D has prior probability 1%, and a draw from the box yields a +, what is the chance that the + came from box D? As the solution shows, such extremely skewed priors and likelihoods may lead to surprising conclusions. False positives. Suppose that a laboratory test on a blood sample yields one of two results, positive or negative. It is found that 95% of people with a particular disease produce a positive result. But 2% of people without the disease will also produce a positive result (a false positive). Suppose that 1% of the population actually has the disease. What is the probability that a person chosen at random from the population will have the disease, given that the person’s blood yields a positive result? Let P(F) denote the proportion of people in the population with characteristic F. Then P(FIG) is the proportion of those in the population with characteristic G who also have characteristic F. The desired probability is P(DI+) where D indicates the disease, and + indicates a positive test result. The data in the problem indicate that P( +ID) = 0.95, P( +IDC) = 0.02, P(D) = 0.01, P(DC) = 0.99. Applying Bayes’ rule with A = +, Bl = D, B2 = DC, gives P(DI+) = P( +ID):;1~1~~~bc)P(DC) (.95) (.01) (.95)(.01) + (.02)(.99) 95 = - :::::::32% 293 Thus only 32% of those persons who produce a positive test result actually have the disease. At first this result seems surprisingly low. The point is that because the</p>
<p>disease is so rare, the number of true positives coming from the few people with the disease is comparable to the number of false positives coming from the many without the disease. Interpretation of conditional probabilities. In applications of Bayes’ rule it is important to keep in mind the interpretation of the various probabilities involved. Typically, the likelihoods P(AIB;) will admit a long-run frequency interpretation. If the prior probabilities P(B;) also have a long-run frequency interpretation, then so too will the conditional probability P(B;IA) given by Bayes’ formula. In Example 3 there were two hypotheses Bl = D that a person was diseased and B2 = DC that a person was not. The observed event was the event A = + of a positive laboratory test. There the conditional probability P(DI+) admitted an empirical interpretation, as that proportion of individuals in the population in question showing a positive test who actually had the disease. This conditional probability also admits a long-run frequency interpretation in terms of repeated sampling of that population, or some other population with the same characteristics assumed in the calculations. Among persons who produce a positive laboratory test, the long-run proportion with the disease will most likely be close to P(DI+) ~ 32%. There are many situations, however, where it is impossible to give a long-run frequency interpretation to the prior probabilities P(B;). The same must then be said of the posterior probabilities P(B; IA) which are calculated in terms of them, even if the likelihoods P(AIB;) have long-run frequency interpretations. Calculations by Bayes’ rule can often be simplified by noting that it is only the ratios P(B;) to P(Bj ) (the prior odds ratios) and the ratios P(AIB;) to P(AIBj ) (the likelihood ratios) which matter. As you can check as an exercise, if the prior odds ratios are written as, say, R; to Rj , and the likelihood ratios as, say, L; to Lj , meaning that for some constant c and for some constant d then the posterior odds ratios P(B;IA) to P(BjIA) are simply R;L; to RjLj , and This is summarized by the following: Bayes’ Rule for Odds posterior odds = prior odds x likelihoods.</p>
<p>Bayes’ rule for odds shows clearly how the prior odds are just as important a factor as the likelihood ratio in computing the posterior odds. If the prior odds don’t make sense in terms of long-run frequencies, neither will the posterior odds. But even if the probabilities don’t admit a long-run frequency interpretation, you might find it useful to regard the probabilities in Bayes’ rule as subjective probabilities. Bayes’ rule then dictates how opinions should be revised in the light of new information, to be consistent with the rules of probability. Here is a typical example. Diagnosis of a particular patient. Suppose a doctor is examining a patient from the population in Example 3. This patient was not chosen at random. He walked into the doctor’s office because he was feeling sick. After examining the patient, but not seeing the result of the blood test, the doctor’s opinion is that there is a 30% chance that the patient has the disease. How should the doctor revise her opinion after seeing a positive blood test? To be consistent with the rules of probability, the doctor should use Bayes’ rule. Now the prior probabilities are P(D) = 30%, P(DC ) = 70% while it might be reasonable to suppose that the likelihoods P(+ID) = 95%, P(+IDC ) = 2% are the same as before. The posterior probability can be calculated as before, using Bayes’ rule, but with the new prior probabilities. In terms of odds, the prior odds in favor of the disease are 3 to 7, the likelihood ratio in favor of the disease is 95 to 2, so the posterior odds in favor are 3 x 95 to 7 x 2, or 285 to 14. So given the positive blood test result, the doctor should revise her opinion and say that the patient has the disease with probability 285 285 285 + 14 = 299 = 0.95317 Notice how working with prior odds of 30 to 70 instead of 1 to 99 has a drastic effect on the conclusion. Provided the prior odds are not heavily against the disease, the evidence of the blood test carries a lot of weight. The likelihood ratio of 95 to 2 overwhelms the doctor’s prior odds of 3 to 7, so there should be little doubt left in the doctor’s mind after seeing the positive blood test. The puzzling question in this kind of application is how does the doctor come up with the odds of 3 to 7 after the medical examination? To come up with such odds, the doctor must make an intuitive judgment based on the whole complex of evidence gained from an examination of the patient. It seems impossible to adequately formalize this process mathematically. The theory does not help the doctor come up with a prior opinion, or explain how</p>
<p>the doctor should revise an OplnlOn in the light of complex information such as is gained from a medical examination. All the theory can do in this context is to suggest how an opinion should be revised in the light of a single additional piece of information, such as the result of a blood test. Notice how the terms prior and posterior are relative terms, like today and tomorrow. The posterior distribution after today’s test will be the prior distribution for tomorrow’s test. So an opinion can be revised repeatedly using Bayes’ rule. At each stage in this process, all probabilities should be computed conditionally on everything that has gone before. Exercises 1.5 1. There are two boxes, the odd box containing 1 black marble and 3 white marbles, and the even box containing 2 black marbles and 4 white marbles. A box is selected at random, and a marble is drawn at random from the selected box. a) What is the probability that the marble is black? b) Given the marble is white, what is the probability that it came from the even box? 2. Polya’s urn scheme. An urn contains 4 white balls and 6 black balls. A ball is chosen at random, and its color noted. The ball is then replaced, along with 3 more balls of the same color (so that there are now 13 balls in the urn). Then another ball is drawn at random from the urn. a) Find the chance that the second ball drawn is white. (Draw an appropriate tree diagram.) b) Given that the second ball drawn is white, what is the probability that the first ball drawn is black? c) Suppose the original contents of the urn are w white and b black balls, and that after a ball is drawn from the urn, it is replaced along with d more balls of the same color. In part a), w was 4, b was 6, and d was 3. Show that the chance that the second ball drawn is white is w~b’ [Note that the probability above does not depend on the value of d.l 3. A manufacturing process produces integrated circuit chips. Over the long run the fraction of bad chips produced by the process is around 20%. Thoroughly testing a chip to determine whether it is good or bad is rather expensive, so a cheap test is tried. All good chips will pass the cheap test, but so will 10%, of the bad chips. a) Given a chip passes the cheap lest, what is the probability that it is a good chip? b) If a company using this manufacturing process sells all chips which pass the cheap test, over the long run what percentage of chips sold will be bad? 4. A digital communications system consists of a transmitter and a receiver. During each short transmission interval the transmitter sends a signal which is to be interpreted as a zero, or it sends a different signal which is to be interpreted as a one. At the end of each interval, the receiver makes its best guess at what was transmitted. Consider the events:</p>
<p>To = {Transmitter sends O}, Ro = {Receiver concludes that a 0 was sent}, Tl = {Transmitter sends I}, Rl = {Receiver concludes that a 1 was sent}. Assume that P(RoITo) = 0.99, P(R1ITI) = 0.98, and P(T1) = 0.5. Find: a) the probability of a transmission error given RI ; b) the overall probability of a transmission error. c) Repeat a) and b) assuming P(TI ) = 0.8 instead of 0.5. 5. False diagnosis. The fraction of persons in a population who have a certain disease is 0.01. A diagnostic test is available to test for the disease. But for a healthy person the chance of being falsely diagnosed as having the disease is 0.05, while for someone with the disease the chance of being falsely diagnosed as healthy is 0.2. Suppose the test is performed on a person selected at random from the population. a) What is the probability that the test shows a positive result (meaning the person is diagnosed as diseased, perhaps correctly, perhaps not)? b) What is the probability that the person selected at random is one who has the disease but is diagnosed healthy? c) What is the probability that the person is correctly diagnosed and is healthy? d) Suppose the test shows a positive result. What is the probability that the person tested actually has the disease? e) Do the above probabilities admit a long-run frequency interpretation? Explain. 6. An experimenter observes the occurrence of an event A as the result of a particular experiment. There are three different hypotheses, HI, H2, and H3 , which the experimenter regards as the only possible explanations of the occurrence of A. Under hypothesis HI, the experiment should produce the result A about 10% of the time over the long run, under H2 about 1% of the time, and under H3 about 39% of the time. Having observed A, the experimenter decides that H3 is the most likely explanation, and that the probability that H3 is true is 39% = 78%. 10% + 1% + 39% a) What assumption is the experimenter implicitly making? b) Does the probability 78% admit a long-run frequency interpretation? c) Suppose the experiment is a laboratory test on a blood sample from an individual chosen at random from a particular population. The hypothesis Hi is that the individual’s blood is of some particular type i. Over the whole population it is known that the proportion of individuals with blood of type 1 is 50%, the proportion with type 2 blood is 45%, and the remaining proportion is type 3. Revise the experimenter’s calculation of the probability of H3 given A, so that it admits a long-run frequency interpretation. Is H3 still the most likely hypothesis given A? 7. Guessing what box. Consider a game as in Examples 1 and 2, where I pick one of the three boxes, then you guess which box I picked after seeing the color of a ball drawn at random from the box. Then you learn whether your guess was right or wrong. Suppose we play the game over and over, replacing the ball drawn and mixing up the balls between plays. Your objective is to guess the box correctly as often as possible.</p>
<p>a) Suppose you know that I pick a box each time at random (probability 1/3 for each box). And suppose you adopt the strategy of guessing the box with highest posterior probability given the observed color, as described in Example 1, in case the observed color is white. About what proportion of the time do you expect to be right over the long run? b) Could you do any better by another guessing strategy? Explain. c) Suppose you use guessing strategy found in a), but I was in fact randomizing the choice of the box each time, with probabilities (1/2,1/4,1/4) instead of (1/3,1/3,1/3). Now how would your strategy perform over the long run? d) Suppose you knew I was either randomizing with probabilities (1/3,1/3,1/3)’ or with probabilities (1/2,1/4,1/4). How could you learn which I was doing? How should you respond, and how would your response perform over the long run? 8. Optimal strategies for guessing what box. (Continuation of Exercise 7, due to David Blackwell.) The question now arises: What randomizing strategy should I use to make it as hard as possible for you to guess correctly? Consider what happens if I use the (f:J, f:J, ~) strategy, and answer the following questions: a) What box should you guess if you see a black ball? b) What box should you guess if you see a white balJ? c) What is your overall chance of winning? You should conclude that with this strategy, your chance of winning is at most f:J, no matter what you do. Moreover, you have a strategy which guarantees you this chance of winning, no matter what randomization I use. It is the following: If black, guess 1 with probability ~, 2 with probability ~, and 3 with probability O. If white, guess 1 with probability 0, 2 with probability H, and 3 with probability K d) Check that using this strategy, you win with probability f:J, no matter what box I piCk. According to the above analysis, I can limit your chance of winning to f:J by a good choice of strategy, and you can guarantee that chance of winning by a good choice of strategy. The fraction f:J is called the value of the above game, where it is understood that the payoff to you is 1 for guessing correctly, 0 otherwise. Optimal strategies of the type discussed above and a resulting value can be defined for a large class of games between two players called zero-sum games. For further discussion consult books on game theory. 9. A box contains three “shapes”, as described in Example 1.3.3. One of the shapes is a fair die, and lands flat with probability 1/3. The other two shapes land flat with probabilities 1/2 and 2/3, respectively. a) One of the three shapes will be chosen at random, and rolled. What is the chance that the number rolled is 6? b) Given that the number rolled is 6, what is the chance that the fair die was chosen?</p>
<p>1.6 Sequences of Events This section is concerned with how to calculate probabilities of events determined by a sequence of outcomes. All that is involved is repeated application of the basic addition and multiplication rules of probability. The first step is a calculation of the probability of an intersection of three events A, B, and C. This event, which occurs if all three of the events occur may be written as ABC = (AB)C. The chance of this event can be computed by using the multiplication rule twice: P(ABC) = P(AB)P(CIAB) = P(A)P(BIA)P(ClAB). FIGURE 1. Tree diagram for the multiplication rule for three events. Repeating this argument shows that for n events, AI”’” An, the probability that every one of these events occurs is a product of n factors. Multiplication Rule for n Events In words, if PI = P(AI) is the probability of the first event, P2 = P(A2IAI) is the probability of the second event given that the first event has occurred, P3 = P(A3IAIA2) is the probability of the third event given that the first two events have occurred, and so on, then the probability that n events AI, ... , An all occur is the product PI x P2 X ... X Pn. This multiplication rule is used to specify the probabilities of paths in a tree diagram. Probabilities of various events of interest can then be found by adding the probabilities over appropriate sets of paths. This technique is illustrated by the following examples.</p>
<p>Example 1. Problem. Solution. Section 1.6. Sequences of Events 57 Completion by stages. A contractor is planning a construction project to be completed in three stages. The contractor figures that CD the chance that the first stage will be completed on time is 0.7. (ii) given that the first stage is completed on time, the chance that the second stage will be completed on time is 0.8. (iii) given that both the first and second stages are completed on time, the chance that the third stage will be completed on time is 0.9. To be consistent, what should the contractor calculate is the chance that all three stages will be completed on time? Let Ci be the event that the ith stage is completed on time, and let Li be the event that stage i is late (the complement of Ci ). The data of the problem are represented in the following tree diagram: The event that all three stages are completed on time is the event C1C2C3 . By the multiplication rule, Note. The data determine the probability of some other events, such as the event that the first and second stages are completed on time but the third is not, which is But the data do not determine the probability of the event that the second stage is late, which is not represented in the diagram. To calculate this probability, it would be necessary to know P(L2ILd, the chance that the second stage is late given that the first stage is late. Then P(L2 ) could be obtained by the rule of average conditional probabilities.</p>
<p>Example 2. Problem 1. Solution. Problem 2. Solution. The geometric distribution. A symmetric die has proportion p of its faces painted white and proportion q of its faces painted black, where q = 1 - p.&nbsp;The die is rolled until the first time a white face shows up. What is the chance that this takes three or less rolls? Assume that, no matter how the die may have landed in previous rolls, the die shows white on each roll with probability p and black with probability q. The problem can then be represented as follows by a tree diagram. The dots indicate that the diagram could be continued in the same way for rolls 4, 5, 6, and so on, but the outcomes of these rolls are not relevant to the problem. The event {white in 3 or less rolls} is represented by three branches of the tree, the first ending at WI on rollI, the second ending at W2 on roll 2, and the third ending at W3 on roll 3. These three branches represent three mutually exclusive ways that the event {white in 3 or less rolls} could happen. The probability of each branch is the product of probabilities along the branches. Thus P(white in 3 or less rolls) = P(Wd + P(BI W2 ) + P(BI B2 W3 ) = p+ qp+ q2p = (1 + q + q2)p What is the chance that it takes four or more rolls to get a white face? This looks as if you have to think about the part of the diagram labeled· . ” representing what might happen if you rolled the die 4 times, 5 times, 6 times, and so on. But there is no need to face this infinite sequence of possible outcomes. The event that it takes 4 or more rolls to get a white face is the complement of the event that it takes three or less rolls to get a white face. Therefore P(4 or more rolls to get white) = 1 - P(white in 3 or less rolls) by the solution to the previous problem</p>
<p>Discussion. Section 1.6. Sequences of Events 59 If you substitute p = 1 - q in this formula and simplify, it reduces to simply q3. To understand why, notice that the event that it takes four or more rolls to get white is simply the event that the first three rolls are black. And the probability of this event is q3, from the tree diagram. This gives the simplest solution to both problems above. As a numerical example, for an ordinary six-sided die, with face 6 white, and the rest black, so p = 1/6, q = 5/6, P(4 or more rolls to get a six) = q3 = (5/6)3 = 125/216:::::: 0.58 P(3 or less rolls to get a six) = 1 - q3 = 1 - (5/6)3 = 91/216 :::::: 0.42 The tree diagram shows that the distribution of the number of rolls required to get a white face is as follows: number of rolls 1 2 3 ... k . .. probability p qp q2p .. . qk-lp ... This is the geometric distribution with parameter p, studied further in Section 3.4. Figure 2 shows the histogram of this distribution for p = 1/6, q = 5/6. FIGURE 2. Geometric distribution of the number of fair die rolls to get a 6. Each bar of the histogram is 5/6 the height of the bar to its left</p>
<p>Example 3. Problem. Solution. The gambler’s rule. Suppose you playa game over and over again, each time with chance 1/ N of winning the game, no matter what the results of previous games. How many times n must you play to have a better than 50% chance of at least one win in the n games? It seems intuitive that n must be comparable in magnitude to N, but just what fraction of N is not clear without calculation. According to a very old gambler’s rule, n is about (2/3)N. To check this, notice that P(at least one win in n games) = 1 - P(no win in n games) We are looking for the least n such that 1- ( 1-- &gt;- 1 )n 1 N 2’ i.e., For small N you can find n by repeated multiplication by (1- 1/ N) until the product is less than 1/2, and check that the gambler’s rule holds. For larger N this becomes tedious. It is more efficient to take logarithms, and to look for the least n such that Keep in mind that both sides are now negative. To find this integer n, first find n*, perhaps not an integer, such that n* log (1- ~) = log (~), that is, n* = log (~) flog (1- ~) So the desired n is the least integer greater than n*. You can check that n* is so close to 2N /3 for small values of N that n is also the least integer greater than 2N /3 for N = 1,2, ... , 27. This rule breaks down for N = 28, but the fraction n/ N stays quite close to 2/3 as N -7 00. To understand why, take logarithms to the base e. (See the appendix on exponents and logarithms.) Then there is the approximation log (1 + z) ‘” z as z -7 0 Apply this to z = -1/ N as N -7 00 to get n’” n* ’” log (~) / (-~) = Nlog(2) where the symbol”, indicates asymptotic equivalence as N -7 00, meaning the ratio of the two sides tends to 1 as N -7 00. So the asymptotic ratio of n to N is log (2) &gt;=::;j 0.69 &gt;=::;j 2/3</p>
<p>Example 4. Problem. Solution. Remark. Section 1 .6. Sequences of Events 61 Probability of a flush. Suppose that a five-card hand is dealt from a well-shuffled deck of 52 cards. What is the probability that the hand is a flush Call cards of the same suit)? A flush could be a flush of spades (S), a flush of hearts (H), a flush of diamonds CD), or a flush of clubs (C). These are four mutually exclusive and equally likely cases. The way to get a spade flush is suggested by the following diagram, with Si representing the event that the ith card dealt is a spade: The conditional probabilities in the diagram were obtained from the usual assumptions of a well-shuffled deck: (D the first card is equally likely to be any of the 52 cards in the deck; (ii) given the first card, the second is equally likely to be any of the 51 left; (iii) given the first two cards, the third is equally likely to be any of the 50 left; and so on. To illustrate, (iii) implies P(S3IS1S2) = 11/50, because given that the first and second cards are spades, no matter what spades they are, there are 11 spades left among the 50 remaining cards in the deck. Using the multiplication rule, 13 12 11 10 9 P(Spade flush) = - x - x - x - x - 52 51 50 49 48 Therefore 12 11 10 9 P(flush) = 4P(Spade flush) = - x - x - x - = 0.00198 51 50 49 48 The probability of any particular sequence of 5 cards can be calculated using the multiplication rule. You could think of this in terms of a huge tree diagram, with 52 branches for the first card, each of these branching into 51 possibilities for the second card, each of these branching into 50 possibilities for the third card, and so on. Each path in the tree would then represent a possible sequence of 5 cards. The probability of any particular sequence being dealt, for example (J\), K., 2\), 30, 50), meaning</p>
<p>the first card is the Jack of Hearts, the next is the King of Spades, and so on, would be 1 1 111 -x-x-x-x52 51 50 49 48 the same for all possible sequences (called permutations) of 5 of the 52 cards. This serves as the basic assumption for calculating probabilities of other types of card hands, by a counting method explained in Chapter 2. The birthday problem. Suppose there are n students in a class. What is the probability that at least two students in the class have the same birthday? The first step is to think how you would determine whether or not this event has occurred for a particular class of students. Here is a natural method. First order the students in some arbitrary way, say alphabetically, then go through the list of students’ birthdays in that order, and check whether or not each birthday is one that has appeared previously. If you find a repeat birthday in this process, stop. There are at least two students in the class with the same birthday. But if you get right through the list of n students, with no repeats, then no two students in the class have the same birthday. Let Rj be the event that the checking process stops with a repeat birthday at the jth student on the list, and let Dj be the event that the first j birthdays are different. The event Bn that there are at least two students in the class with the same birthday is the event R2 U R3 U ... U Rn that the checking process stops with a repeat at some stage j :S n as you go through the list. The events R2 , ... , Rn are represented in the following diagram. They are mutually exclusive, so But it is simpler to calculate the probability of Bn from its complement, which is Dn, the event that all n birthdays are different: R2 R3 1 2 3 n-l 365 365 365 365 364 363 362 365-(n-l) 365 D2 365 D3 365 Dn- 1 L-_----“3”“65”--_+- Dn The conditional probabilities in the diagram are based on the following assumption:</p>
<p>No matter what the birthdays of the first j - 1 students, the birthday of the jth student is equally likely to be anyone of the 365 days of the year. This ignores leap years, and seasonal variation in birth rates. But it can be shown that neither of these considerations affects the answer very much. Granted the assumption. we have P(D2 ) = - = 1 - - 364 ( 1) 365 365 because no matter what the birthday of the first student, there are 364 out of 365 possible birthdays for the second student which would make the first and second students have different birthdays. If the first j birthdays are different, then so are the first i for every i &lt; j, so Dj C Di . Thus D2D3··· Dj = Dj , and because given D j, the first j students have j different birthdays, and no matter what these birthdays are, the next student must have one of the remaining 365 - j birthdays for Dj+l to occur. Multiplying these conditional probabilities along the branch of the diagram through D2, D3, ... , Dn gives P(D n ) = P(D2D3 ... D n ) = (1 -_1 365 ) (1 - ~) 365 ... (1 - ~) 365 where the last factor comes from taking j = n - 1 in the formula for P(Dj+lIDj). Figure 3 displays the graph of P(Bn) = 1 - P(Dn) against n, obtained by this formula. The most amazing thing is how rapidly P(Bn) increases as n increases. The least n such that P(Bn) &gt; 1/2 is n = 23: and P(Bn ) is up to about 94% by n = 45, and 99.8% by n = 65. Above n = 70, P(Bn ) is so close to 1 that there is no point in plotting the graph. The value of P(Bn) is shown in the graph by the height of the dot above n on the horizontal scale. These dots are closely approximated by the smooth curve drawn just below the dots. This curve is obtained by calculating the product using logarithms and the tangent approximation log (1 + z) ’” z for small z, as in Example 3. Thus</p>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>